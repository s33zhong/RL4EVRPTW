{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T22:15:20.877005Z",
     "start_time": "2024-11-17T22:15:16.125128Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from rl4co.envs import CVRPTWEnv, EVRPTWEnv \n",
    "from rl4co.models import AttentionModelPolicy, REINFORCE, SymNCO, PPO, POMO, RewardConstrainedPOMO\n",
    "from rl4co.utils.trainer import RL4COTrainer\n",
    "from rl4co.utils.callbacks.reward_check import RewardLoggingCallback, get_reward_and_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bd30bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\rl4co\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import rl4co\n",
    "print(rl4co.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec844555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\python311.zip\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\DLLs\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\n",
      "\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\win32\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\win32\\lib\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\Pythonwin\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\setuptools\\_vendor\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.remove(r\"c:\\users\\hyosi\\onedrive\\ut\\2024 fall\\mie1666\\project\\code\\rl4evrptw\\rl4co\")\n",
    "\n",
    "for path in sys.path:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e58a04627ea0a434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T22:15:21.147698Z",
     "start_time": "2024-11-17T22:15:20.877005Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\torchrl\\data\\tensor_specs.py:5464: DeprecationWarning: The BoundedTensorSpec has been deprecated and will be removed in v0.7. Please use Bounded instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\torchrl\\data\\tensor_specs.py:5464: DeprecationWarning: The UnboundedDiscreteTensorSpec has been deprecated and will be removed in v0.7. Please use Unbounded instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\torchrl\\data\\tensor_specs.py:5464: DeprecationWarning: The CompositeSpec has been deprecated and will be removed in v0.7. Please use Composite instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\torchrl\\data\\tensor_specs.py:5464: DeprecationWarning: The UnboundedContinuousTensorSpec has been deprecated and will be removed in v0.7. Please use Unbounded instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "def enforce_reproducibility(seed):\n",
    "    import random\n",
    "    import os \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # NVIDIA's CUDA Basic Linear Algebra Subroutines library\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "vehicle_capacity = 1.25\n",
    "\n",
    "# [num_loc, num_station, num_ev]\n",
    "settings =[[10, 3, 3], [20, 3, 3], [50, 6, 6], [100, 12,12]]\n",
    "hard_envs = []\n",
    "td_tests = []   # Hard env setting for test (cf. get_action_mask() is different)\n",
    "for num_loc, num_station, num_ev in settings:\n",
    "    enforce_reproducibility(0)\n",
    "    env = EVRPTWEnv(generator_params={'num_loc': num_loc, \n",
    "                                        'num_station': num_station,\n",
    "                                        'vehicle_limit': num_ev,\n",
    "                                        'vehicle_speed': 5,\n",
    "                                        'vehicle_capacity': vehicle_capacity,\n",
    "                                        'max_time': 1,\n",
    "                                        'horizon': 1,\n",
    "                                        'fuel_consumption_rate': 0.25,\n",
    "                                        'inverse_recharge_rate': 0.25})\n",
    "    hard_envs.append(env)\n",
    "    td_init = env.reset(batch_size=[100]).to(device)\n",
    "    td_tests.append(td_init)\n",
    "\n",
    "soft_envs = []\n",
    "for num_loc, num_station, num_ev in settings:\n",
    "    enforce_reproducibility(0)\n",
    "    env = EVRPTWEnv(generator_params={'num_loc': num_loc, \n",
    "                                        'num_station': num_station,\n",
    "                                        'vehicle_limit': num_ev,\n",
    "                                        'vehicle_speed': 5,\n",
    "                                        'vehicle_capacity': vehicle_capacity,\n",
    "                                        'max_time': 1,\n",
    "                                        'horizon': 1,\n",
    "                                        'fuel_consumption_rate': 0.25,\n",
    "                                        'inverse_recharge_rate': 0.25})\n",
    "    env.soft = True ## Soft setting\n",
    "    soft_envs.append(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2b92290e4554f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T22:15:25.146517Z",
     "start_time": "2024-11-17T22:15:24.187177Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCH = 25\n",
    "BATCH_SIZE = 512\n",
    "TRAIN_DATA_SIZE = BATCH_SIZE * 200\n",
    "VAL_DATA_SIZE = BATCH_SIZE * 50\n",
    "# MAX_EPOCH = 2\n",
    "# BATCH_SIZE = 10\n",
    "# TRAIN_DATA_SIZE = BATCH_SIZE * 1\n",
    "# VAL_DATA_SIZE = BATCH_SIZE * 1\n",
    "\n",
    "# RCPOMO\n",
    "policy_c1 = AttentionModelPolicy(env_name=soft_envs[0].name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "policy_c2 = AttentionModelPolicy(env_name=soft_envs[1].name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "policy_c5 = AttentionModelPolicy(env_name=soft_envs[2].name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "soft_model_c10 = RewardConstrainedPOMO(soft_envs[0],\n",
    "                policy_c1,\n",
    "                 # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                  \"weight_decay\": 1e-6})\n",
    "\n",
    "soft_model_c20 = RewardConstrainedPOMO(soft_envs[1],\n",
    "                policy_c2,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "soft_model_c50 = RewardConstrainedPOMO(soft_envs[2],\n",
    "                policy_c5,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "hard_model_c10 = POMO(hard_envs[0],\n",
    "                policy_c1,\n",
    "                 # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                  \"weight_decay\": 1e-6})\n",
    "\n",
    "hard_model_c20 = POMO(hard_envs[1],\n",
    "                policy_c2,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "hard_model_c50 = POMO(hard_envs[2],\n",
    "                policy_c5,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83febd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 200/200 [00:51<00:00,  3.86it/s, v_num=569, train/reward=-4.10, train/loss=-1.50, val/reward=-3.76]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\rl4co\\utils\\callbacks\\reward_check.py:55: RuntimeWarning: Mean of empty slice.\n",
      "  epoch_data[f\"C{s}_mean_reward\"] = -rewards_trained[i].mean()\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [00:22<00:00,  8.97it/s, v_num=569, train/reward=-3.58, train/loss=-0.141, val/reward=-3.56] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [00:22<00:00,  8.92it/s, v_num=569, train/reward=-3.58, train/loss=-0.141, val/reward=-3.56]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [00:20<00:00,  9.68it/s, v_num=570, train/reward=-6.67, train/loss=-0.294, val/reward=-5.04]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [00:20<00:00,  9.63it/s, v_num=570, train/reward=-6.67, train/loss=-0.294, val/reward=-5.04]\n"
     ]
    }
   ],
   "source": [
    "scale = [10, 20, 50, 100]\n",
    "\n",
    "# RCPOMO\n",
    "soft_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c1.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_SOFT_RCPOMO_C10\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "soft_trainer_C_STEP.fit(soft_model_c10)\n",
    "\n",
    "# POMO\n",
    "hard_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c1.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_HARD_POMO_C10\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "hard_trainer_C_STEP.fit(hard_model_c10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6c431e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "25Soft_RCPOMO + 25Hard_POMO: Trained with Environment of C=10, S=3, EV=3\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 4.858532\n",
      "Scale: 20 | FeasibleCounts: 98 | Mean Trained Test Cost: 7.898353\n",
      "Scale: 50 | FeasibleCounts: 98 | Mean Trained Test Cost: 16.292616\n",
      "Scale: 100 | FeasibleCounts: 98 | Mean Trained Test Cost: 31.698280\n"
     ]
    }
   ],
   "source": [
    "policy_c1 = policy_c1.to(device)\n",
    "rewards_c_trained, num_c_valid = get_reward_and_check(policy_c1, td_tests, hard_envs)\n",
    "print(\"\\n25Soft_RCPOMO + 25Hard_POMO: Trained with Environment of C=10, S=3, EV=3\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_c_valid[i]} | Mean Trained Test Cost: {-rewards_c_trained[i].mean():3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98a7e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "del soft_trainer_C_STEP, hard_trainer_C_STEP\n",
    "del rewards_c_trained, num_c_valid\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a224fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [00:39<00:00,  5.02it/s, v_num=571, train/reward=-5.54, train/loss=-0.461, val/reward=-5.46]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [00:39<00:00,  5.00it/s, v_num=571, train/reward=-5.54, train/loss=-0.461, val/reward=-5.46]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [00:35<00:00,  5.61it/s, v_num=572, train/reward=-51.5, train/loss=-4.11, val/reward=-7.91]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [00:35<00:00,  5.59it/s, v_num=572, train/reward=-51.5, train/loss=-4.11, val/reward=-7.91]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RCPOMO\n",
    "soft_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c2.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_SOFT_RCPOMO_C20\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "soft_trainer_C_STEP.fit(soft_model_c20)\n",
    "\n",
    "hard_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c2.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_HARD_POMO_C20\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "hard_trainer_C_STEP.fit(hard_model_c20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b630a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "25Soft_RCPOMO + 25Hard_POMO: Trained with Environment of C=20, S=3, EV=3\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 4.752318\n",
      "Scale: 20 | FeasibleCounts: 99 | Mean Trained Test Cost: 7.654899\n",
      "Scale: 50 | FeasibleCounts: 100 | Mean Trained Test Cost: 15.985867\n",
      "Scale: 100 | FeasibleCounts: 98 | Mean Trained Test Cost: 31.536602\n"
     ]
    }
   ],
   "source": [
    "policy_c2 = policy_c2.to(device)\n",
    "rewards_c_trained, num_c_valid = get_reward_and_check(policy_c2, td_tests, hard_envs)\n",
    "print(\"\\n25Soft_RCPOMO + 25Hard_POMO: Trained with Environment of C=20, S=3, EV=3\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_c_valid[i]} | Mean Trained Test Cost: {-rewards_c_trained[i].mean():3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b939bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del soft_trainer_C_STEP, hard_trainer_C_STEP\n",
    "del rewards_c_trained, num_c_valid\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8d3043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [01:55<00:00,  1.74it/s, v_num=573, train/reward=-10.8, train/loss=-1.25, val/reward=-10.6]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [01:55<00:00,  1.73it/s, v_num=573, train/reward=-10.8, train/loss=-1.25, val/reward=-10.6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [01:34<00:00,  2.11it/s, v_num=574, train/reward=-22.5, train/loss=-4.41, val/reward=-15.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [01:34<00:00,  2.11it/s, v_num=574, train/reward=-22.5, train/loss=-4.41, val/reward=-15.2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RCPOMO\n",
    "soft_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c5.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_SOFT_RCPOMO_C50\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "soft_trainer_C_STEP.fit(soft_model_c50)\n",
    "\n",
    "hard_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c5.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_HARD_POMO_C50\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "hard_trainer_C_STEP.fit(hard_model_c50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d20f059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "25Soft_RCPOMO + 25Hard_POMO: Trained with Environment of C=50, S=6, EV=6\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 5.177136\n",
      "Scale: 20 | FeasibleCounts: 99 | Mean Trained Test Cost: 7.793137\n",
      "Scale: 50 | FeasibleCounts: 100 | Mean Trained Test Cost: 15.010887\n",
      "Scale: 100 | FeasibleCounts: 100 | Mean Trained Test Cost: 26.946321\n"
     ]
    }
   ],
   "source": [
    "policy_c5 = policy_c5.to(device)\n",
    "rewards_c_trained, num_c_valid = get_reward_and_check(policy_c5, td_tests, hard_envs)\n",
    "print(\"\\n25Soft_RCPOMO + 25Hard_POMO: Trained with Environment of C=50, S=6, EV=6\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_c_valid[i]} | Mean Trained Test Cost: {-rewards_c_trained[i].mean():3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3884d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "del soft_trainer_C_STEP, hard_trainer_C_STEP\n",
    "del rewards_c_trained, num_c_valid\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "812b110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCH = 25\n",
    "BATCH_SIZE_100 = 128\n",
    "TRAIN_DATA_SIZE_100 = BATCH_SIZE_100 * 200\n",
    "VAL_DATA_SIZE_100 = BATCH_SIZE_100 * 50\n",
    "\n",
    "# RCPOMO\n",
    "policy_c100 = AttentionModelPolicy(env_name=soft_envs[3].name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "soft_model_c100 = RewardConstrainedPOMO(soft_envs[3],\n",
    "                policy_c100,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE_100,\n",
    "                train_data_size=TRAIN_DATA_SIZE_100,\n",
    "                val_data_size=VAL_DATA_SIZE_100,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "# POMO\n",
    "hard_model_c100 = POMO(hard_envs[3],\n",
    "                policy_c100,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE_100,\n",
    "                train_data_size=TRAIN_DATA_SIZE_100,\n",
    "                val_data_size=VAL_DATA_SIZE_100,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b742846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [03:13<00:00,  1.03it/s, v_num=575, train/reward=-19.5, train/loss=-2.86, val/reward=-18.9] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [03:14<00:00,  1.03it/s, v_num=575, train/reward=-19.5, train/loss=-2.86, val/reward=-18.9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [02:34<00:00,  1.30it/s, v_num=576, train/reward=-22.3, train/loss=-2.44, val/reward=-21.5]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [02:34<00:00,  1.29it/s, v_num=576, train/reward=-22.3, train/loss=-2.44, val/reward=-21.5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RCPOMO\n",
    "soft_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c100.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_SOFT_RCPOMO_C100\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "soft_trainer_C_STEP.fit(soft_model_c100)\n",
    "\n",
    "# POMO\n",
    "hard_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c100.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_HARD_POMO_C100\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "hard_trainer_C_STEP.fit(hard_model_c100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08ae3598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "25Soft_RCPOMO + 25Hard_POMO: Trained with Environment of C=100, S=12, EV=12\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 4.955643\n",
      "Scale: 20 | FeasibleCounts: 79 | Mean Trained Test Cost: 7.069251\n",
      "Scale: 50 | FeasibleCounts: 92 | Mean Trained Test Cost: 13.122625\n",
      "Scale: 100 | FeasibleCounts: 100 | Mean Trained Test Cost: 21.177683\n"
     ]
    }
   ],
   "source": [
    "policy_c100 = policy_c100.to(device)\n",
    "rewards_c_trained, num_c_valid = get_reward_and_check(policy_c100, td_tests, hard_envs)\n",
    "print(\"\\n25Soft_RCPOMO + 25Hard_POMO: Trained with Environment of C=100, S=12, EV=12\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_c_valid[i]} | Mean Trained Test Cost: {-rewards_c_trained[i].mean():3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62355511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl4co",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
