{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T22:15:20.877005Z",
     "start_time": "2024-11-17T22:15:16.125128Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from rl4co.envs import CVRPTWEnv, EVRPTWEnv \n",
    "from rl4co.models import AttentionModelPolicy, REINFORCE, SymNCO, PPO, POMO, RewardConstrainedPOMO\n",
    "from rl4co.utils.trainer import RL4COTrainer\n",
    "from rl4co.utils.callbacks.reward_check import RewardLoggingCallback, get_reward_and_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bd30bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\rl4co\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import rl4co\n",
    "print(rl4co.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec844555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\python311.zip\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\DLLs\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\n",
      "\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\win32\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\win32\\lib\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\Pythonwin\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\setuptools\\_vendor\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.remove(r\"c:\\users\\hyosi\\onedrive\\ut\\2024 fall\\mie1666\\project\\code\\rl4evrptw\\rl4co\")\n",
    "\n",
    "for path in sys.path:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e58a04627ea0a434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T22:15:21.147698Z",
     "start_time": "2024-11-17T22:15:20.877005Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\torchrl\\data\\tensor_specs.py:5464: DeprecationWarning: The BoundedTensorSpec has been deprecated and will be removed in v0.7. Please use Bounded instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\torchrl\\data\\tensor_specs.py:5464: DeprecationWarning: The UnboundedDiscreteTensorSpec has been deprecated and will be removed in v0.7. Please use Unbounded instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\torchrl\\data\\tensor_specs.py:5464: DeprecationWarning: The CompositeSpec has been deprecated and will be removed in v0.7. Please use Composite instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\torchrl\\data\\tensor_specs.py:5464: DeprecationWarning: The UnboundedContinuousTensorSpec has been deprecated and will be removed in v0.7. Please use Unbounded instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "def enforce_reproducibility(seed):\n",
    "    import random\n",
    "    import os \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # NVIDIA's CUDA Basic Linear Algebra Subroutines library\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "vehicle_capacity = 1.25\n",
    "\n",
    "# [num_loc, num_station, num_ev]\n",
    "settings =[[10, 3, 3], [20, 3, 3], [50, 6, 6], [100, 12,12]]\n",
    "hard_envs = []\n",
    "td_tests = []   # Hard env setting for test (cf. get_action_mask() is different)\n",
    "for num_loc, num_station, num_ev in settings:\n",
    "    enforce_reproducibility(0)\n",
    "    env = EVRPTWEnv(generator_params={'num_loc': num_loc, \n",
    "                                        'num_station': num_station,\n",
    "                                        'vehicle_limit': num_ev,\n",
    "                                        'vehicle_speed': 5,\n",
    "                                        'vehicle_capacity': vehicle_capacity,\n",
    "                                        'max_time': 1,\n",
    "                                        'horizon': 1,\n",
    "                                        'fuel_consumption_rate': 0.25,\n",
    "                                        'inverse_recharge_rate': 0.25})\n",
    "    hard_envs.append(env)\n",
    "    td_init = env.reset(batch_size=[100]).to(device)\n",
    "    td_tests.append(td_init)\n",
    "\n",
    "soft_envs = []\n",
    "for num_loc, num_station, num_ev in settings:\n",
    "    enforce_reproducibility(0)\n",
    "    env = EVRPTWEnv(generator_params={'num_loc': num_loc, \n",
    "                                        'num_station': num_station,\n",
    "                                        'vehicle_limit': num_ev,\n",
    "                                        'vehicle_speed': 5,\n",
    "                                        'vehicle_capacity': vehicle_capacity,\n",
    "                                        'max_time': 1,\n",
    "                                        'horizon': 1,\n",
    "                                        'fuel_consumption_rate': 0.25,\n",
    "                                        'inverse_recharge_rate': 0.25})\n",
    "    env.soft = True ## Soft setting\n",
    "    soft_envs.append(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2b92290e4554f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T22:15:25.146517Z",
     "start_time": "2024-11-17T22:15:24.187177Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCH = 25\n",
    "BATCH_SIZE = 512\n",
    "TRAIN_DATA_SIZE = BATCH_SIZE * 200\n",
    "VAL_DATA_SIZE = BATCH_SIZE * 50\n",
    "# MAX_EPOCH = 2\n",
    "# BATCH_SIZE = 10\n",
    "# TRAIN_DATA_SIZE = BATCH_SIZE * 1\n",
    "# VAL_DATA_SIZE = BATCH_SIZE * 1\n",
    "\n",
    "# RCPOMO\n",
    "policy_c1 = AttentionModelPolicy(env_name=soft_envs[0].name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "policy_c2 = AttentionModelPolicy(env_name=soft_envs[1].name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "policy_c5 = AttentionModelPolicy(env_name=soft_envs[2].name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "soft_model_c10 = RewardConstrainedPOMO(soft_envs[0],\n",
    "                policy_c1,\n",
    "                 # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                  \"weight_decay\": 1e-6})\n",
    "\n",
    "soft_model_c20 = RewardConstrainedPOMO(soft_envs[1],\n",
    "                policy_c2,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "soft_model_c50 = RewardConstrainedPOMO(soft_envs[2],\n",
    "                policy_c5,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "hard_model_c10 = RewardConstrainedPOMO(hard_envs[0],\n",
    "                policy_c1,\n",
    "                 # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                  \"weight_decay\": 1e-6})\n",
    "\n",
    "hard_model_c20 = RewardConstrainedPOMO(hard_envs[1],\n",
    "                policy_c2,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "hard_model_c50 = RewardConstrainedPOMO(hard_envs[2],\n",
    "                policy_c5,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83febd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 200/200 [00:25<00:00,  7.96it/s, v_num=552, train/reward=-6.69, train/loss=-8.60, val/reward=-3.88]Callback is finished\n",
      "Epoch 1: 100%|██████████| 200/200 [00:25<00:00,  7.79it/s, v_num=552, train/reward=-3.90, train/loss=-0.351, val/reward=-3.79]Callback is finished\n",
      "Epoch 2: 100%|██████████| 200/200 [00:26<00:00,  7.46it/s, v_num=552, train/reward=-3.80, train/loss=-0.297, val/reward=-3.74]Callback is finished\n",
      "Epoch 3: 100%|██████████| 200/200 [00:26<00:00,  7.47it/s, v_num=552, train/reward=-3.92, train/loss=-0.541, val/reward=-3.73] Callback is finished\n",
      "Epoch 3: 100%|██████████| 200/200 [00:27<00:00,  7.26it/s, v_num=552, train/reward=-3.92, train/loss=-0.541, val/reward=-3.73]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\rl4co\\utils\\callbacks\\reward_check.py:55: RuntimeWarning: Mean of empty slice.\n",
      "  epoch_data[f\"C{s}_mean_reward\"] = -rewards_trained[i].mean()\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 200/200 [00:27<00:00,  7.33it/s, v_num=552, train/reward=-3.77, train/loss=-0.218, val/reward=-3.69]Callback is finished\n",
      "Epoch 5: 100%|██████████| 200/200 [00:26<00:00,  7.54it/s, v_num=552, train/reward=-3.69, train/loss=-0.214, val/reward=-3.64]Callback is finished\n",
      "Epoch 6: 100%|██████████| 200/200 [00:26<00:00,  7.62it/s, v_num=552, train/reward=-3.67, train/loss=-0.203, val/reward=-3.66]Callback is finished\n",
      "Epoch 7: 100%|██████████| 200/200 [00:26<00:00,  7.67it/s, v_num=552, train/reward=-3.67, train/loss=-0.193, val/reward=-3.66]Callback is finished\n",
      "Epoch 8: 100%|██████████| 200/200 [00:25<00:00,  7.89it/s, v_num=552, train/reward=-3.70, train/loss=-0.193, val/reward=-3.64] Callback is finished\n",
      "Epoch 9: 100%|██████████| 200/200 [00:27<00:00,  7.34it/s, v_num=552, train/reward=-3.67, train/loss=-0.176, val/reward=-3.65]Callback is finished\n",
      "Epoch 10: 100%|██████████| 200/200 [00:26<00:00,  7.51it/s, v_num=552, train/reward=-3.64, train/loss=-0.153, val/reward=-3.61]Callback is finished\n",
      "Epoch 11: 100%|██████████| 200/200 [00:25<00:00,  7.73it/s, v_num=552, train/reward=-3.64, train/loss=-0.162, val/reward=-3.61]Callback is finished\n",
      "Epoch 12: 100%|██████████| 200/200 [00:25<00:00,  7.98it/s, v_num=552, train/reward=-3.63, train/loss=-0.136, val/reward=-3.62]Callback is finished\n",
      "Epoch 13: 100%|██████████| 200/200 [00:26<00:00,  7.64it/s, v_num=552, train/reward=-3.57, train/loss=-0.131, val/reward=-3.60]Callback is finished\n",
      "Epoch 14: 100%|██████████| 200/200 [00:25<00:00,  7.78it/s, v_num=552, train/reward=-3.64, train/loss=-0.133, val/reward=-3.60]Callback is finished\n",
      "Epoch 15: 100%|██████████| 200/200 [00:26<00:00,  7.49it/s, v_num=552, train/reward=-3.59, train/loss=-0.125, val/reward=-3.58] Callback is finished\n",
      "Epoch 16: 100%|██████████| 200/200 [00:26<00:00,  7.50it/s, v_num=552, train/reward=-3.60, train/loss=-0.147, val/reward=-3.60] Callback is finished\n",
      "Epoch 17: 100%|██████████| 200/200 [00:26<00:00,  7.51it/s, v_num=552, train/reward=-3.64, train/loss=-0.123, val/reward=-3.60]Callback is finished\n",
      "Epoch 18: 100%|██████████| 200/200 [00:25<00:00,  7.95it/s, v_num=552, train/reward=-3.60, train/loss=-0.115, val/reward=-3.56]Callback is finished\n",
      "Epoch 19: 100%|██████████| 200/200 [00:24<00:00,  8.12it/s, v_num=552, train/reward=-3.63, train/loss=-0.124, val/reward=-3.61]Callback is finished\n",
      "Epoch 20: 100%|██████████| 200/200 [00:26<00:00,  7.63it/s, v_num=552, train/reward=-3.64, train/loss=-0.123, val/reward=-3.60] Callback is finished\n",
      "Epoch 21: 100%|██████████| 200/200 [00:25<00:00,  7.81it/s, v_num=552, train/reward=-3.61, train/loss=-0.128, val/reward=-3.57] Callback is finished\n",
      "Epoch 22: 100%|██████████| 200/200 [00:25<00:00,  7.87it/s, v_num=552, train/reward=-3.60, train/loss=-0.116, val/reward=-3.57] Callback is finished\n",
      "Epoch 23: 100%|██████████| 200/200 [00:25<00:00,  7.99it/s, v_num=552, train/reward=-3.60, train/loss=-0.12, val/reward=-3.57]  Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [00:25<00:00,  7.85it/s, v_num=552, train/reward=-3.58, train/loss=-0.121, val/reward=-3.55] Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [00:26<00:00,  7.62it/s, v_num=552, train/reward=-3.58, train/loss=-0.121, val/reward=-3.55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [00:26<00:00,  7.58it/s, v_num=552, train/reward=-3.58, train/loss=-0.121, val/reward=-3.55]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 200/200 [00:22<00:00,  8.73it/s, v_num=553, train/reward=-11.0, train/loss=-1.13, val/reward=-5.14]Callback is finished\n",
      "Epoch 1: 100%|██████████| 200/200 [00:21<00:00,  9.15it/s, v_num=553, train/reward=-15.8, train/loss=-1.08, val/reward=-5.23]  Callback is finished\n",
      "Epoch 2: 100%|██████████| 200/200 [00:21<00:00,  9.11it/s, v_num=553, train/reward=-6.95, train/loss=-0.172, val/reward=-5.08] Callback is finished\n",
      "Epoch 3: 100%|██████████| 200/200 [00:21<00:00,  9.30it/s, v_num=553, train/reward=-7.62, train/loss=-0.072, val/reward=-5.09]  Callback is finished\n",
      "Epoch 4: 100%|██████████| 200/200 [00:22<00:00,  8.78it/s, v_num=553, train/reward=-8.33, train/loss=-0.517, val/reward=-5.02] Callback is finished\n",
      "Epoch 5: 100%|██████████| 200/200 [00:22<00:00,  8.86it/s, v_num=553, train/reward=-12.4, train/loss=-0.53, val/reward=-4.97]  Callback is finished\n",
      "Epoch 6: 100%|██████████| 200/200 [00:21<00:00,  9.37it/s, v_num=553, train/reward=-8.83, train/loss=-0.513, val/reward=-4.88] Callback is finished\n",
      "Epoch 7: 100%|██████████| 200/200 [00:22<00:00,  8.76it/s, v_num=553, train/reward=-5.99, train/loss=-0.381, val/reward=-4.98] Callback is finished\n",
      "Epoch 8: 100%|██████████| 200/200 [00:22<00:00,  8.72it/s, v_num=553, train/reward=-8.02, train/loss=0.765, val/reward=-5.05]  Callback is finished\n",
      "Epoch 9: 100%|██████████| 200/200 [00:20<00:00,  9.82it/s, v_num=553, train/reward=-9.84, train/loss=0.0207, val/reward=-5.16]  Callback is finished\n",
      "Epoch 10: 100%|██████████| 200/200 [00:21<00:00,  9.20it/s, v_num=553, train/reward=-7.28, train/loss=-0.773, val/reward=-5.05]  Callback is finished\n",
      "Epoch 11: 100%|██████████| 200/200 [00:22<00:00,  8.96it/s, v_num=553, train/reward=-7.07, train/loss=0.942, val/reward=-5.05]  Callback is finished\n",
      "Epoch 12: 100%|██████████| 200/200 [00:22<00:00,  9.08it/s, v_num=553, train/reward=-6.46, train/loss=-0.616, val/reward=-5.02]  Callback is finished\n",
      "Epoch 13: 100%|██████████| 200/200 [00:22<00:00,  8.84it/s, v_num=553, train/reward=-6.55, train/loss=-0.441, val/reward=-5.12]  Callback is finished\n",
      "Epoch 14: 100%|██████████| 200/200 [00:22<00:00,  8.90it/s, v_num=553, train/reward=-7.68, train/loss=-0.193, val/reward=-4.97] Callback is finished\n",
      "Epoch 15: 100%|██████████| 200/200 [00:20<00:00,  9.74it/s, v_num=553, train/reward=-9.33, train/loss=0.520, val/reward=-5.07]  Callback is finished\n",
      "Epoch 16: 100%|██████████| 200/200 [00:21<00:00,  9.31it/s, v_num=553, train/reward=-10.3, train/loss=-0.782, val/reward=-4.90]  Callback is finished\n",
      "Epoch 17: 100%|██████████| 200/200 [00:23<00:00,  8.67it/s, v_num=553, train/reward=-6.34, train/loss=-0.618, val/reward=-4.89]  Callback is finished\n",
      "Epoch 18: 100%|██████████| 200/200 [00:21<00:00,  9.13it/s, v_num=553, train/reward=-6.81, train/loss=0.0714, val/reward=-4.96] Callback is finished\n",
      "Epoch 19: 100%|██████████| 200/200 [00:22<00:00,  8.99it/s, v_num=553, train/reward=-7.33, train/loss=0.826, val/reward=-5.04]   Callback is finished\n",
      "Epoch 20: 100%|██████████| 200/200 [00:21<00:00,  9.13it/s, v_num=553, train/reward=-8.83, train/loss=0.413, val/reward=-5.03]   Callback is finished\n",
      "Epoch 21: 100%|██████████| 200/200 [00:20<00:00,  9.63it/s, v_num=553, train/reward=-7.09, train/loss=0.432, val/reward=-4.99]  Callback is finished\n",
      "Epoch 22: 100%|██████████| 200/200 [00:21<00:00,  9.27it/s, v_num=553, train/reward=-9.23, train/loss=-0.996, val/reward=-5.00] Callback is finished\n",
      "Epoch 23: 100%|██████████| 200/200 [00:20<00:00,  9.63it/s, v_num=553, train/reward=-6.47, train/loss=0.451, val/reward=-4.88]  Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [00:20<00:00,  9.74it/s, v_num=553, train/reward=-6.65, train/loss=-0.239, val/reward=-4.99] Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [00:21<00:00,  9.46it/s, v_num=553, train/reward=-6.65, train/loss=-0.239, val/reward=-4.99]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [00:21<00:00,  9.40it/s, v_num=553, train/reward=-6.65, train/loss=-0.239, val/reward=-4.99]\n"
     ]
    }
   ],
   "source": [
    "scale = [10, 20, 50, 100]\n",
    "\n",
    "# RCPOMO\n",
    "soft_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c1.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_SOFT_RCPOMO_C10\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "soft_trainer_C_STEP.fit(soft_model_c10)\n",
    "\n",
    "hard_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c1.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_HARD_RCPOMO_C10\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "hard_trainer_C_STEP.fit(hard_model_c10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d474c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RCPOMO: Trained with Environment of C=10, S=3, EV=3\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 4.827217\n",
      "Scale: 20 | FeasibleCounts: 95 | Mean Trained Test Cost: 7.735532\n",
      "Scale: 50 | FeasibleCounts: 95 | Mean Trained Test Cost: 16.282394\n",
      "Scale: 100 | FeasibleCounts: 99 | Mean Trained Test Cost: 31.068134\n"
     ]
    }
   ],
   "source": [
    "policy_c1 = policy_c1.to(device)\n",
    "rewards_c_trained, num_c_valid = get_reward_and_check(policy_c1, td_tests, hard_envs)\n",
    "print(\"\\nRCPOMO: Trained with Environment of C=10, S=3, EV=3\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_c_valid[i]} | Mean Trained Test Cost: {-rewards_c_trained[i].mean():3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98a7e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "del soft_trainer_C_STEP, hard_trainer_C_STEP\n",
    "del rewards_c_trained, num_c_valid\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a224fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 200/200 [00:44<00:00,  4.54it/s, v_num=554, train/reward=-6.57, train/loss=-2.85, val/reward=-5.91]Callback is finished\n",
      "Epoch 1: 100%|██████████| 200/200 [00:41<00:00,  4.81it/s, v_num=554, train/reward=-8.30, train/loss=-9.83, val/reward=-6.16]Callback is finished\n",
      "Epoch 2: 100%|██████████| 200/200 [00:41<00:00,  4.79it/s, v_num=554, train/reward=-6.90, train/loss=-4.31, val/reward=-6.15] Callback is finished\n",
      "Epoch 3: 100%|██████████| 200/200 [00:42<00:00,  4.70it/s, v_num=554, train/reward=-6.40, train/loss=-1.14, val/reward=-5.97] Callback is finished\n",
      "Epoch 4: 100%|██████████| 200/200 [00:40<00:00,  5.00it/s, v_num=554, train/reward=-6.15, train/loss=-1.38, val/reward=-5.83] Callback is finished\n",
      "Epoch 5: 100%|██████████| 200/200 [00:41<00:00,  4.87it/s, v_num=554, train/reward=-6.01, train/loss=-0.857, val/reward=-5.79]Callback is finished\n",
      "Epoch 6: 100%|██████████| 200/200 [00:40<00:00,  4.93it/s, v_num=554, train/reward=-6.02, train/loss=-0.686, val/reward=-5.87]Callback is finished\n",
      "Epoch 7: 100%|██████████| 200/200 [00:39<00:00,  5.04it/s, v_num=554, train/reward=-5.85, train/loss=-0.645, val/reward=-5.71]Callback is finished\n",
      "Epoch 8: 100%|██████████| 200/200 [00:40<00:00,  4.93it/s, v_num=554, train/reward=-5.85, train/loss=-0.592, val/reward=-5.73]Callback is finished\n",
      "Epoch 9: 100%|██████████| 200/200 [00:40<00:00,  4.90it/s, v_num=554, train/reward=-6.17, train/loss=-3.00, val/reward=-5.76] Callback is finished\n",
      "Epoch 10: 100%|██████████| 200/200 [00:41<00:00,  4.88it/s, v_num=554, train/reward=-5.83, train/loss=-0.53, val/reward=-5.63] Callback is finished\n",
      "Epoch 11: 100%|██████████| 200/200 [00:40<00:00,  4.91it/s, v_num=554, train/reward=-5.79, train/loss=-0.618, val/reward=-5.68]Callback is finished\n",
      "Epoch 12: 100%|██████████| 200/200 [00:40<00:00,  4.91it/s, v_num=554, train/reward=-5.88, train/loss=-1.46, val/reward=-5.67] Callback is finished\n",
      "Epoch 13: 100%|██████████| 200/200 [00:39<00:00,  5.04it/s, v_num=554, train/reward=-5.91, train/loss=-0.668, val/reward=-5.65]Callback is finished\n",
      "Epoch 14: 100%|██████████| 200/200 [00:39<00:00,  5.05it/s, v_num=554, train/reward=-5.67, train/loss=-0.532, val/reward=-5.63]Callback is finished\n",
      "Epoch 15: 100%|██████████| 200/200 [00:40<00:00,  5.00it/s, v_num=554, train/reward=-5.96, train/loss=-1.75, val/reward=-5.61] Callback is finished\n",
      "Epoch 16: 100%|██████████| 200/200 [00:38<00:00,  5.22it/s, v_num=554, train/reward=-5.57, train/loss=-0.454, val/reward=-5.51]Callback is finished\n",
      "Epoch 17: 100%|██████████| 200/200 [00:39<00:00,  5.09it/s, v_num=554, train/reward=-5.61, train/loss=-0.509, val/reward=-5.49]Callback is finished\n",
      "Epoch 18: 100%|██████████| 200/200 [00:39<00:00,  5.06it/s, v_num=554, train/reward=-5.64, train/loss=-0.592, val/reward=-5.51]Callback is finished\n",
      "Epoch 19: 100%|██████████| 200/200 [00:38<00:00,  5.26it/s, v_num=554, train/reward=-5.62, train/loss=-0.484, val/reward=-5.55]Callback is finished\n",
      "Epoch 20: 100%|██████████| 200/200 [00:39<00:00,  5.00it/s, v_num=554, train/reward=-5.67, train/loss=-1.39, val/reward=-5.51] Callback is finished\n",
      "Epoch 21: 100%|██████████| 200/200 [00:39<00:00,  5.03it/s, v_num=554, train/reward=-5.84, train/loss=-1.50, val/reward=-5.47] Callback is finished\n",
      "Epoch 22: 100%|██████████| 200/200 [00:39<00:00,  5.07it/s, v_num=554, train/reward=-5.58, train/loss=-0.441, val/reward=-5.51]Callback is finished\n",
      "Epoch 23: 100%|██████████| 200/200 [00:41<00:00,  4.82it/s, v_num=554, train/reward=-5.69, train/loss=-0.722, val/reward=-5.49]Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [00:38<00:00,  5.14it/s, v_num=554, train/reward=-5.69, train/loss=-2.37, val/reward=-5.44] Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [00:39<00:00,  5.03it/s, v_num=554, train/reward=-5.69, train/loss=-2.37, val/reward=-5.44]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [00:39<00:00,  5.02it/s, v_num=554, train/reward=-5.69, train/loss=-2.37, val/reward=-5.44]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 200/200 [00:35<00:00,  5.59it/s, v_num=555, train/reward=-88.8, train/loss=-30.3, val/reward=-8.03]Callback is finished\n",
      "Epoch 1: 100%|██████████| 200/200 [00:36<00:00,  5.47it/s, v_num=555, train/reward=-75.7, train/loss=4.120, val/reward=-8.21] Callback is finished\n",
      "Epoch 2: 100%|██████████| 200/200 [00:34<00:00,  5.84it/s, v_num=555, train/reward=-63.7, train/loss=-10.1, val/reward=-8.24]  Callback is finished\n",
      "Epoch 3: 100%|██████████| 200/200 [00:35<00:00,  5.63it/s, v_num=555, train/reward=-60.9, train/loss=-4.87, val/reward=-8.15] Callback is finished\n",
      "Epoch 4: 100%|██████████| 200/200 [00:35<00:00,  5.60it/s, v_num=555, train/reward=-60.8, train/loss=-4.06, val/reward=-8.02] Callback is finished\n",
      "Epoch 5: 100%|██████████| 200/200 [00:37<00:00,  5.34it/s, v_num=555, train/reward=-60.5, train/loss=5.070, val/reward=-8.08] Callback is finished\n",
      "Epoch 6: 100%|██████████| 200/200 [00:35<00:00,  5.56it/s, v_num=555, train/reward=-48.5, train/loss=-7.56, val/reward=-8.13] Callback is finished\n",
      "Epoch 7: 100%|██████████| 200/200 [00:35<00:00,  5.61it/s, v_num=555, train/reward=-58.8, train/loss=-6.42, val/reward=-8.02] Callback is finished\n",
      "Epoch 8: 100%|██████████| 200/200 [00:35<00:00,  5.57it/s, v_num=555, train/reward=-67.2, train/loss=-13.6, val/reward=-8.07]Callback is finished\n",
      "Epoch 9: 100%|██████████| 200/200 [00:35<00:00,  5.61it/s, v_num=555, train/reward=-56.1, train/loss=-3.05, val/reward=-7.97] Callback is finished\n",
      "Epoch 10: 100%|██████████| 200/200 [00:34<00:00,  5.72it/s, v_num=555, train/reward=-53.4, train/loss=-2.69, val/reward=-8.10]   Callback is finished\n",
      "Epoch 11: 100%|██████████| 200/200 [00:34<00:00,  5.76it/s, v_num=555, train/reward=-76.4, train/loss=-7.76, val/reward=-8.06]  Callback is finished\n",
      "Epoch 12: 100%|██████████| 200/200 [00:34<00:00,  5.77it/s, v_num=555, train/reward=-51.0, train/loss=-7.56, val/reward=-8.02]  Callback is finished\n",
      "Epoch 13: 100%|██████████| 200/200 [00:35<00:00,  5.58it/s, v_num=555, train/reward=-46.8, train/loss=-2.62, val/reward=-8.06] Callback is finished\n",
      "Epoch 14: 100%|██████████| 200/200 [00:35<00:00,  5.61it/s, v_num=555, train/reward=-59.0, train/loss=-0.458, val/reward=-7.98] Callback is finished\n",
      "Epoch 15: 100%|██████████| 200/200 [00:36<00:00,  5.48it/s, v_num=555, train/reward=-58.2, train/loss=-4.50, val/reward=-7.97]  Callback is finished\n",
      "Epoch 16: 100%|██████████| 200/200 [00:35<00:00,  5.60it/s, v_num=555, train/reward=-48.7, train/loss=0.467, val/reward=-8.07]  Callback is finished\n",
      "Epoch 17: 100%|██████████| 200/200 [00:35<00:00,  5.66it/s, v_num=555, train/reward=-53.1, train/loss=-7.28, val/reward=-8.01]  Callback is finished\n",
      "Epoch 18: 100%|██████████| 200/200 [00:35<00:00,  5.57it/s, v_num=555, train/reward=-65.9, train/loss=-2.17, val/reward=-8.12] Callback is finished\n",
      "Epoch 19: 100%|██████████| 200/200 [00:35<00:00,  5.71it/s, v_num=555, train/reward=-65.8, train/loss=0.194, val/reward=-8.17] Callback is finished\n",
      "Epoch 20: 100%|██████████| 200/200 [00:34<00:00,  5.75it/s, v_num=555, train/reward=-65.0, train/loss=-9.98, val/reward=-8.03]  Callback is finished\n",
      "Epoch 21: 100%|██████████| 200/200 [00:38<00:00,  5.19it/s, v_num=555, train/reward=-50.2, train/loss=-16.3, val/reward=-8.40] Callback is finished\n",
      "Epoch 22: 100%|██████████| 200/200 [00:36<00:00,  5.54it/s, v_num=555, train/reward=-53.7, train/loss=1.120, val/reward=-7.96] Callback is finished\n",
      "Epoch 23: 100%|██████████| 200/200 [00:36<00:00,  5.44it/s, v_num=555, train/reward=-60.1, train/loss=-4.17, val/reward=-7.98] Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [00:36<00:00,  5.46it/s, v_num=555, train/reward=-49.7, train/loss=-8.21, val/reward=-8.01] Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [00:37<00:00,  5.37it/s, v_num=555, train/reward=-49.7, train/loss=-8.21, val/reward=-8.01]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [00:37<00:00,  5.36it/s, v_num=555, train/reward=-49.7, train/loss=-8.21, val/reward=-8.01]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RCPOMO\n",
    "soft_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c2.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_SOFT_RCPOMO_C20\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "soft_trainer_C_STEP.fit(soft_model_c20)\n",
    "\n",
    "hard_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c2.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_HARD_RCPOMO_C20\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "hard_trainer_C_STEP.fit(hard_model_c20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04145ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RCPOMO: Trained with Environment of C=20, S=3, EV=3\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 4.863012\n",
      "Scale: 20 | FeasibleCounts: 100 | Mean Trained Test Cost: 7.804768\n",
      "Scale: 50 | FeasibleCounts: 100 | Mean Trained Test Cost: 16.240088\n",
      "Scale: 100 | FeasibleCounts: 97 | Mean Trained Test Cost: 32.686348\n"
     ]
    }
   ],
   "source": [
    "policy_c2 = policy_c2.to(device)\n",
    "rewards_c_trained, num_c_valid = get_reward_and_check(policy_c2, td_tests, hard_envs)\n",
    "print(\"\\nRCPOMO: Trained with Environment of C=20, S=3, EV=3\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_c_valid[i]} | Mean Trained Test Cost: {-rewards_c_trained[i].mean():3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b939bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del soft_trainer_C_STEP, hard_trainer_C_STEP\n",
    "del rewards_c_trained, num_c_valid\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8d3043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 200/200 [02:00<00:00,  1.66it/s, v_num=556, train/reward=-14.1, train/loss=-6.77, val/reward=-12.5]Callback is finished\n",
      "Epoch 1: 100%|██████████| 200/200 [01:53<00:00,  1.76it/s, v_num=556, train/reward=-13.0, train/loss=-3.17, val/reward=-12.1]Callback is finished\n",
      "Epoch 2: 100%|██████████| 200/200 [01:55<00:00,  1.73it/s, v_num=556, train/reward=-12.6, train/loss=-2.56, val/reward=-11.9]Callback is finished\n",
      "Epoch 3: 100%|██████████| 200/200 [01:55<00:00,  1.74it/s, v_num=556, train/reward=-12.0, train/loss=-2.55, val/reward=-11.4]Callback is finished\n",
      "Epoch 4: 100%|██████████| 200/200 [01:54<00:00,  1.75it/s, v_num=556, train/reward=-11.9, train/loss=-2.62, val/reward=-11.4]Callback is finished\n",
      "Epoch 5: 100%|██████████| 200/200 [01:55<00:00,  1.73it/s, v_num=556, train/reward=-11.8, train/loss=-2.07, val/reward=-11.3]Callback is finished\n",
      "Epoch 6: 100%|██████████| 200/200 [01:54<00:00,  1.75it/s, v_num=556, train/reward=-11.5, train/loss=-2.41, val/reward=-11.0]Callback is finished\n",
      "Epoch 7: 100%|██████████| 200/200 [01:54<00:00,  1.74it/s, v_num=556, train/reward=-11.3, train/loss=-1.85, val/reward=-10.9]Callback is finished\n",
      "Epoch 8: 100%|██████████| 200/200 [01:56<00:00,  1.71it/s, v_num=556, train/reward=-11.7, train/loss=-2.96, val/reward=-11.3]Callback is finished\n",
      "Epoch 9: 100%|██████████| 200/200 [01:54<00:00,  1.74it/s, v_num=556, train/reward=-12.7, train/loss=-6.34, val/reward=-11.1]Callback is finished\n",
      "Epoch 10: 100%|██████████| 200/200 [01:54<00:00,  1.75it/s, v_num=556, train/reward=-11.1, train/loss=-1.57, val/reward=-10.7]Callback is finished\n",
      "Epoch 11: 100%|██████████| 200/200 [01:55<00:00,  1.73it/s, v_num=556, train/reward=-11.3, train/loss=-2.20, val/reward=-11.0]Callback is finished\n",
      "Epoch 12: 100%|██████████| 200/200 [01:57<00:00,  1.70it/s, v_num=556, train/reward=-11.1, train/loss=-1.72, val/reward=-10.7]Callback is finished\n",
      "Epoch 13: 100%|██████████| 200/200 [01:54<00:00,  1.74it/s, v_num=556, train/reward=-11.2, train/loss=-2.72, val/reward=-10.7]Callback is finished\n",
      "Epoch 14: 100%|██████████| 200/200 [01:55<00:00,  1.73it/s, v_num=556, train/reward=-11.0, train/loss=-1.26, val/reward=-10.7]Callback is finished\n",
      "Epoch 15: 100%|██████████| 200/200 [01:54<00:00,  1.74it/s, v_num=556, train/reward=-11.0, train/loss=-2.17, val/reward=-10.7]Callback is finished\n",
      "Epoch 16: 100%|██████████| 200/200 [01:54<00:00,  1.74it/s, v_num=556, train/reward=-11.2, train/loss=-2.69, val/reward=-10.8]Callback is finished\n",
      "Epoch 17: 100%|██████████| 200/200 [01:54<00:00,  1.75it/s, v_num=556, train/reward=-10.9, train/loss=-1.55, val/reward=-10.8]Callback is finished\n",
      "Epoch 18: 100%|██████████| 200/200 [01:54<00:00,  1.74it/s, v_num=556, train/reward=-11.1, train/loss=-3.61, val/reward=-10.6]Callback is finished\n",
      "Epoch 19: 100%|██████████| 200/200 [01:53<00:00,  1.76it/s, v_num=556, train/reward=-10.7, train/loss=-1.44, val/reward=-10.5]Callback is finished\n",
      "Epoch 20: 100%|██████████| 200/200 [01:55<00:00,  1.73it/s, v_num=556, train/reward=-10.8, train/loss=-1.07, val/reward=-10.5] Callback is finished\n",
      "Epoch 21: 100%|██████████| 200/200 [01:53<00:00,  1.76it/s, v_num=556, train/reward=-10.7, train/loss=-1.28, val/reward=-10.5]Callback is finished\n",
      "Epoch 22: 100%|██████████| 200/200 [01:54<00:00,  1.75it/s, v_num=556, train/reward=-10.7, train/loss=-1.12, val/reward=-10.5]Callback is finished\n",
      "Epoch 23: 100%|██████████| 200/200 [01:54<00:00,  1.74it/s, v_num=556, train/reward=-10.7, train/loss=-1.13, val/reward=-10.5] Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [01:55<00:00,  1.74it/s, v_num=556, train/reward=-10.7, train/loss=-1.91, val/reward=-10.5] Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [01:55<00:00,  1.73it/s, v_num=556, train/reward=-10.7, train/loss=-1.91, val/reward=-10.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [01:55<00:00,  1.73it/s, v_num=556, train/reward=-10.7, train/loss=-1.91, val/reward=-10.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 200/200 [01:35<00:00,  2.10it/s, v_num=557, train/reward=-27.6, train/loss=-15.7, val/reward=-16.0]Callback is finished\n",
      "Epoch 1: 100%|██████████| 200/200 [01:34<00:00,  2.13it/s, v_num=557, train/reward=-25.8, train/loss=-8.98, val/reward=-15.8] Callback is finished\n",
      "Epoch 2: 100%|██████████| 200/200 [01:33<00:00,  2.13it/s, v_num=557, train/reward=-24.0, train/loss=0.395, val/reward=-15.6] Callback is finished\n",
      "Epoch 3: 100%|██████████| 200/200 [01:32<00:00,  2.15it/s, v_num=557, train/reward=-28.2, train/loss=-9.67, val/reward=-15.2] Callback is finished\n",
      "Epoch 4: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s, v_num=557, train/reward=-24.7, train/loss=-2.06, val/reward=-15.7] Callback is finished\n",
      "Epoch 5: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s, v_num=557, train/reward=-25.1, train/loss=-8.01, val/reward=-16.2] Callback is finished\n",
      "Epoch 6: 100%|██████████| 200/200 [01:33<00:00,  2.15it/s, v_num=557, train/reward=-23.0, train/loss=-1.40, val/reward=-15.7]  Callback is finished\n",
      "Epoch 7: 100%|██████████| 200/200 [01:33<00:00,  2.15it/s, v_num=557, train/reward=-22.0, train/loss=-2.27, val/reward=-15.4] Callback is finished\n",
      "Epoch 8: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s, v_num=557, train/reward=-24.4, train/loss=-2.47, val/reward=-15.5] Callback is finished\n",
      "Epoch 9: 100%|██████████| 200/200 [01:33<00:00,  2.14it/s, v_num=557, train/reward=-21.9, train/loss=-2.31, val/reward=-15.1]  Callback is finished\n",
      "Epoch 10: 100%|██████████| 200/200 [01:35<00:00,  2.09it/s, v_num=557, train/reward=-24.0, train/loss=-3.85, val/reward=-15.1] Callback is finished\n",
      "Epoch 11: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s, v_num=557, train/reward=-21.0, train/loss=-6.44, val/reward=-15.3] Callback is finished\n",
      "Epoch 12: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s, v_num=557, train/reward=-24.6, train/loss=-5.72, val/reward=-15.3] Callback is finished\n",
      "Epoch 13: 100%|██████████| 200/200 [01:33<00:00,  2.13it/s, v_num=557, train/reward=-20.9, train/loss=-6.24, val/reward=-15.3]   Callback is finished\n",
      "Epoch 14: 100%|██████████| 200/200 [01:33<00:00,  2.14it/s, v_num=557, train/reward=-24.5, train/loss=-3.19, val/reward=-15.5] Callback is finished\n",
      "Epoch 15: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s, v_num=557, train/reward=-23.9, train/loss=-2.53, val/reward=-15.2]  Callback is finished\n",
      "Epoch 16: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s, v_num=557, train/reward=-25.7, train/loss=-1.41, val/reward=-15.2] Callback is finished\n",
      "Epoch 17: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s, v_num=557, train/reward=-24.5, train/loss=-3.44, val/reward=-15.7] Callback is finished\n",
      "Epoch 18: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s, v_num=557, train/reward=-21.1, train/loss=-6.35, val/reward=-15.3] Callback is finished\n",
      "Epoch 19: 100%|██████████| 200/200 [01:30<00:00,  2.20it/s, v_num=557, train/reward=-20.3, train/loss=-9.84, val/reward=-15.5] Callback is finished\n",
      "Epoch 20: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s, v_num=557, train/reward=-21.1, train/loss=-3.68, val/reward=-15.3] Callback is finished\n",
      "Epoch 21: 100%|██████████| 200/200 [01:34<00:00,  2.11it/s, v_num=557, train/reward=-20.1, train/loss=-4.23, val/reward=-14.9] Callback is finished\n",
      "Epoch 22: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s, v_num=557, train/reward=-22.0, train/loss=-2.55, val/reward=-15.4] Callback is finished\n",
      "Epoch 23: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s, v_num=557, train/reward=-21.8, train/loss=-7.48, val/reward=-15.1]    Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s, v_num=557, train/reward=-22.7, train/loss=-6.37, val/reward=-15.4] Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [01:32<00:00,  2.17it/s, v_num=557, train/reward=-22.7, train/loss=-6.37, val/reward=-15.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s, v_num=557, train/reward=-22.7, train/loss=-6.37, val/reward=-15.4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RCPOMO\n",
    "soft_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c5.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_SOFT_RCPOMO_C50\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "soft_trainer_C_STEP.fit(soft_model_c50)\n",
    "\n",
    "hard_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c5.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_HARD_RCPOMO_C50\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "hard_trainer_C_STEP.fit(hard_model_c50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b9babf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RCPOMO: Trained with Environment of C=50, S=6, EV=6\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 4.846461\n",
      "Scale: 20 | FeasibleCounts: 98 | Mean Trained Test Cost: 7.474066\n",
      "Scale: 50 | FeasibleCounts: 100 | Mean Trained Test Cost: 14.989614\n",
      "Scale: 100 | FeasibleCounts: 100 | Mean Trained Test Cost: 26.575089\n"
     ]
    }
   ],
   "source": [
    "policy_c5 = policy_c5.to(device)\n",
    "rewards_c_trained, num_c_valid = get_reward_and_check(policy_c5, td_tests, hard_envs)\n",
    "print(\"\\nRCPOMO: Trained with Environment of C=50, S=6, EV=6\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_c_valid[i]} | Mean Trained Test Cost: {-rewards_c_trained[i].mean():3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3884d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "del soft_trainer_C_STEP, hard_trainer_C_STEP\n",
    "del rewards_c_trained, num_c_valid\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "812b110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCH = 25\n",
    "BATCH_SIZE_100 = 128\n",
    "TRAIN_DATA_SIZE_100 = BATCH_SIZE_100 * 200\n",
    "VAL_DATA_SIZE_100 = BATCH_SIZE_100 * 50\n",
    "\n",
    "# RCPOMO\n",
    "policy_c100 = AttentionModelPolicy(env_name=soft_envs[3].name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "soft_model_c100 = RewardConstrainedPOMO(soft_envs[3],\n",
    "                policy_c100,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE_100,\n",
    "                train_data_size=TRAIN_DATA_SIZE_100,\n",
    "                val_data_size=VAL_DATA_SIZE_100,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "hard_model_c100 = RewardConstrainedPOMO(hard_envs[3],\n",
    "                policy_c100,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE_100,\n",
    "                train_data_size=TRAIN_DATA_SIZE_100,\n",
    "                val_data_size=VAL_DATA_SIZE_100,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b742846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 200/200 [03:33<00:00,  0.93it/s, v_num=558, train/reward=-27.3, train/loss=-10.8, val/reward=-24.9]Callback is finished\n",
      "Epoch 1: 100%|██████████| 200/200 [03:15<00:00,  1.02it/s, v_num=558, train/reward=-25.5, train/loss=-8.76, val/reward=-22.5]Callback is finished\n",
      "Epoch 2: 100%|██████████| 200/200 [03:20<00:00,  1.00it/s, v_num=558, train/reward=-23.0, train/loss=-5.17, val/reward=-22.3]Callback is finished\n",
      "Epoch 3: 100%|██████████| 200/200 [03:19<00:00,  1.00it/s, v_num=558, train/reward=-22.7, train/loss=-4.56, val/reward=-21.8]Callback is finished\n",
      "Epoch 4: 100%|██████████| 200/200 [03:19<00:00,  1.00it/s, v_num=558, train/reward=-22.9, train/loss=-4.02, val/reward=-21.6]Callback is finished\n",
      "Epoch 5: 100%|██████████| 200/200 [03:18<00:00,  1.01it/s, v_num=558, train/reward=-21.9, train/loss=-4.17, val/reward=-21.3]Callback is finished\n",
      "Epoch 6: 100%|██████████| 200/200 [03:16<00:00,  1.02it/s, v_num=558, train/reward=-21.4, train/loss=-3.20, val/reward=-20.5]Callback is finished\n",
      "Epoch 7: 100%|██████████| 200/200 [03:17<00:00,  1.01it/s, v_num=558, train/reward=-21.7, train/loss=-3.63, val/reward=-20.9]Callback is finished\n",
      "Epoch 8: 100%|██████████| 200/200 [03:18<00:00,  1.01it/s, v_num=558, train/reward=-21.1, train/loss=-4.05, val/reward=-20.5]Callback is finished\n",
      "Epoch 9: 100%|██████████| 200/200 [03:19<00:00,  1.00it/s, v_num=558, train/reward=-20.9, train/loss=-3.92, val/reward=-20.1]Callback is finished\n",
      "Epoch 10: 100%|██████████| 200/200 [03:15<00:00,  1.02it/s, v_num=558, train/reward=-20.6, train/loss=-3.32, val/reward=-20.3]Callback is finished\n",
      "Epoch 11: 100%|██████████| 200/200 [03:14<00:00,  1.03it/s, v_num=558, train/reward=-21.0, train/loss=-5.66, val/reward=-20.2]Callback is finished\n",
      "Epoch 12: 100%|██████████| 200/200 [03:16<00:00,  1.02it/s, v_num=558, train/reward=-21.7, train/loss=-5.11, val/reward=-20.9]Callback is finished\n",
      "Epoch 13: 100%|██████████| 200/200 [03:14<00:00,  1.03it/s, v_num=558, train/reward=-20.7, train/loss=-3.84, val/reward=-20.0]Callback is finished\n",
      "Epoch 14: 100%|██████████| 200/200 [03:14<00:00,  1.03it/s, v_num=558, train/reward=-21.0, train/loss=-7.23, val/reward=-20.0]Callback is finished\n",
      "Epoch 15: 100%|██████████| 200/200 [03:18<00:00,  1.01it/s, v_num=558, train/reward=-20.4, train/loss=-5.50, val/reward=-19.9]  Callback is finished\n",
      "Epoch 16: 100%|██████████| 200/200 [03:14<00:00,  1.03it/s, v_num=558, train/reward=-20.9, train/loss=-4.09, val/reward=-19.8]Callback is finished\n",
      "Epoch 17: 100%|██████████| 200/200 [03:16<00:00,  1.02it/s, v_num=558, train/reward=-21.9, train/loss=-8.11, val/reward=-20.9]Callback is finished\n",
      "Epoch 18: 100%|██████████| 200/200 [03:14<00:00,  1.03it/s, v_num=558, train/reward=-21.5, train/loss=-6.08, val/reward=-20.0]Callback is finished\n",
      "Epoch 19: 100%|██████████| 200/200 [03:12<00:00,  1.04it/s, v_num=558, train/reward=-20.3, train/loss=-2.91, val/reward=-19.6]Callback is finished\n",
      "Epoch 20: 100%|██████████| 200/200 [03:37<00:00,  0.92it/s, v_num=558, train/reward=-2.01e+5, train/loss=4.46e+6, val/reward=-21.9]Callback is finished\n",
      "Epoch 21: 100%|██████████| 200/200 [03:19<00:00,  1.00it/s, v_num=558, train/reward=-20.0, train/loss=-2.72, val/reward=-19.6]     Callback is finished\n",
      "Epoch 22: 100%|██████████| 200/200 [03:16<00:00,  1.02it/s, v_num=558, train/reward=-20.5, train/loss=-2.98, val/reward=-19.7] Callback is finished\n",
      "Epoch 23: 100%|██████████| 200/200 [03:12<00:00,  1.04it/s, v_num=558, train/reward=-20.7, train/loss=-3.06, val/reward=-19.4]Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [03:13<00:00,  1.03it/s, v_num=558, train/reward=-20.9, train/loss=-4.48, val/reward=-19.9]Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [03:14<00:00,  1.03it/s, v_num=558, train/reward=-20.9, train/loss=-4.48, val/reward=-19.9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [03:14<00:00,  1.03it/s, v_num=558, train/reward=-20.9, train/loss=-4.48, val/reward=-19.9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 200/200 [02:46<00:00,  1.20it/s, v_num=559, train/reward=-28.9, train/loss=-6.45, val/reward=-26.5]Callback is finished\n",
      "Epoch 1: 100%|██████████| 200/200 [02:38<00:00,  1.26it/s, v_num=559, train/reward=-25.8, train/loss=-2.28, val/reward=-24.7] Callback is finished\n",
      "Epoch 2: 100%|██████████| 200/200 [02:37<00:00,  1.27it/s, v_num=559, train/reward=-24.8, train/loss=-2.05, val/reward=-23.9] Callback is finished\n",
      "Epoch 3: 100%|██████████| 200/200 [02:36<00:00,  1.27it/s, v_num=559, train/reward=-24.4, train/loss=-2.34, val/reward=-24.0] Callback is finished\n",
      "Epoch 4: 100%|██████████| 200/200 [02:40<00:00,  1.25it/s, v_num=559, train/reward=-24.6, train/loss=-3.07, val/reward=-23.3] Callback is finished\n",
      "Epoch 5: 100%|██████████| 200/200 [02:37<00:00,  1.27it/s, v_num=559, train/reward=-24.3, train/loss=-2.01, val/reward=-23.3] Callback is finished\n",
      "Epoch 6: 100%|██████████| 200/200 [02:39<00:00,  1.25it/s, v_num=559, train/reward=-23.5, train/loss=-3.10, val/reward=-22.5] Callback is finished\n",
      "Epoch 7: 100%|██████████| 200/200 [02:37<00:00,  1.27it/s, v_num=559, train/reward=-24.0, train/loss=-3.11, val/reward=-22.7]  Callback is finished\n",
      "Epoch 8: 100%|██████████| 200/200 [02:37<00:00,  1.27it/s, v_num=559, train/reward=-24.1, train/loss=-0.148, val/reward=-22.6]Callback is finished\n",
      "Epoch 9: 100%|██████████| 200/200 [02:38<00:00,  1.27it/s, v_num=559, train/reward=-23.3, train/loss=-2.54, val/reward=-22.4] Callback is finished\n",
      "Epoch 10: 100%|██████████| 200/200 [02:35<00:00,  1.29it/s, v_num=559, train/reward=-23.5, train/loss=-2.21, val/reward=-22.4] Callback is finished\n",
      "Epoch 11: 100%|██████████| 200/200 [02:42<00:00,  1.23it/s, v_num=559, train/reward=-22.4, train/loss=-4.16, val/reward=-21.9] Callback is finished\n",
      "Epoch 12: 100%|██████████| 200/200 [02:37<00:00,  1.27it/s, v_num=559, train/reward=-22.7, train/loss=-1.00, val/reward=-21.9] Callback is finished\n",
      "Epoch 13: 100%|██████████| 200/200 [02:38<00:00,  1.26it/s, v_num=559, train/reward=-22.5, train/loss=-1.71, val/reward=-21.7] Callback is finished\n",
      "Epoch 14: 100%|██████████| 200/200 [02:39<00:00,  1.25it/s, v_num=559, train/reward=-22.6, train/loss=0.582, val/reward=-21.7]  Callback is finished\n",
      "Epoch 15: 100%|██████████| 200/200 [02:40<00:00,  1.25it/s, v_num=559, train/reward=-22.7, train/loss=-2.24, val/reward=-21.7] Callback is finished\n",
      "Epoch 16: 100%|██████████| 200/200 [02:40<00:00,  1.25it/s, v_num=559, train/reward=-22.5, train/loss=-1.55, val/reward=-21.7] Callback is finished\n",
      "Epoch 17: 100%|██████████| 200/200 [02:35<00:00,  1.29it/s, v_num=559, train/reward=-22.6, train/loss=-0.539, val/reward=-21.6] Callback is finished\n",
      "Epoch 18: 100%|██████████| 200/200 [02:38<00:00,  1.26it/s, v_num=559, train/reward=-22.1, train/loss=-1.78, val/reward=-21.5]  Callback is finished\n",
      "Epoch 19: 100%|██████████| 200/200 [02:37<00:00,  1.27it/s, v_num=559, train/reward=-22.0, train/loss=-1.40, val/reward=-21.7] Callback is finished\n",
      "Epoch 20: 100%|██████████| 200/200 [02:39<00:00,  1.25it/s, v_num=559, train/reward=-22.5, train/loss=-1.57, val/reward=-21.4] Callback is finished\n",
      "Epoch 21: 100%|██████████| 200/200 [02:37<00:00,  1.27it/s, v_num=559, train/reward=-22.5, train/loss=-1.38, val/reward=-21.7] Callback is finished\n",
      "Epoch 22: 100%|██████████| 200/200 [02:40<00:00,  1.25it/s, v_num=559, train/reward=-21.7, train/loss=-1.05, val/reward=-21.1]  Callback is finished\n",
      "Epoch 23: 100%|██████████| 200/200 [02:37<00:00,  1.27it/s, v_num=559, train/reward=-22.1, train/loss=-2.67, val/reward=-21.3] Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [02:40<00:00,  1.25it/s, v_num=559, train/reward=-21.9, train/loss=-0.458, val/reward=-21.3]Callback is finished\n",
      "Epoch 24: 100%|██████████| 200/200 [02:40<00:00,  1.24it/s, v_num=559, train/reward=-21.9, train/loss=-0.458, val/reward=-21.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 200/200 [02:40<00:00,  1.24it/s, v_num=559, train/reward=-21.9, train/loss=-0.458, val/reward=-21.3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RCPOMO\n",
    "soft_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c100.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_SOFT_RCPOMO_C100\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "soft_trainer_C_STEP.fit(soft_model_c100)\n",
    "\n",
    "hard_trainer_C_STEP = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c100.to(device),\n",
    "            test_data=td_tests,\n",
    "            env_scale=hard_envs,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"25_HARD_RCPOMO_C100\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "hard_trainer_C_STEP.fit(hard_model_c100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5f0dff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RCPOMO: Trained with Environment of C=100, S=12, EV=12\n",
      "Scale: 10 | FeasibleCounts: 94 | Mean Trained Test Cost: 6.650102\n",
      "Scale: 20 | FeasibleCounts: 78 | Mean Trained Test Cost: 6.828812\n",
      "Scale: 50 | FeasibleCounts: 93 | Mean Trained Test Cost: 12.858833\n",
      "Scale: 100 | FeasibleCounts: 100 | Mean Trained Test Cost: 20.862005\n"
     ]
    }
   ],
   "source": [
    "policy_c100 = policy_c100.to(device)\n",
    "rewards_c_trained, num_c_valid = get_reward_and_check(policy_c100, td_tests, hard_envs)\n",
    "print(\"\\nRCPOMO: Trained with Environment of C=100, S=12, EV=12\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_c_valid[i]} | Mean Trained Test Cost: {-rewards_c_trained[i].mean():3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae3598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62355511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl4co",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
