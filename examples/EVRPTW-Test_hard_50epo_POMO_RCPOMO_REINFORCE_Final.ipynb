{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T22:15:20.877005Z",
     "start_time": "2024-11-17T22:15:16.125128Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from rl4co.envs import CVRPTWEnv, EVRPTWEnv \n",
    "from rl4co.models import AttentionModelPolicy, REINFORCE, SymNCO, PPO, POMO, RewardConstrainedPOMO\n",
    "from rl4co.utils.trainer import RL4COTrainer\n",
    "from rl4co.utils.callbacks.reward_check import RewardLoggingCallback, get_reward_and_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bd30bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\rl4co\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import rl4co\n",
    "print(rl4co.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec844555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\python311.zip\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\DLLs\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\n",
      "\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\win32\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\win32\\lib\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\Pythonwin\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\setuptools\\_vendor\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.remove(r\"c:\\users\\hyosi\\onedrive\\ut\\2024 fall\\mie1666\\project\\code\\rl4evrptw\\rl4co\")\n",
    "\n",
    "for path in sys.path:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e58a04627ea0a434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T22:15:21.147698Z",
     "start_time": "2024-11-17T22:15:20.877005Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "def enforce_reproducibility(seed):\n",
    "    import random\n",
    "    import os \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # NVIDIA's CUDA Basic Linear Algebra Subroutines library\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "vehicle_capacity = 1.25\n",
    "\n",
    "enforce_reproducibility(0)\n",
    "env_10 = EVRPTWEnv(generator_params={'num_loc': 10, \n",
    "                                    'num_station': 3,\n",
    "                                    'vehicle_limit': 3,\n",
    "                                    'vehicle_speed': 5,\n",
    "                                    'vehicle_capacity': vehicle_capacity,\n",
    "                                    'max_time': 1,\n",
    "                                    'horizon': 1,\n",
    "                                    'fuel_consumption_rate': 0.25,\n",
    "                                    'inverse_recharge_rate': 0.25})\n",
    "td_10_TEST = env_10.reset(batch_size=[100]).to(device)\n",
    "\n",
    "\n",
    "enforce_reproducibility(0)\n",
    "env_20 = EVRPTWEnv(generator_params={'num_loc': 20, \n",
    "                                     'num_station': 3,\n",
    "                                     'vehicle_limit': 3,\n",
    "                                     'vehicle_speed': 5,\n",
    "                                     'vehicle_capacity': vehicle_capacity,\n",
    "                                     'max_time': 1,\n",
    "                                     'horizon': 1,\n",
    "                                     'fuel_consumption_rate': 0.25,\n",
    "                                     'inverse_recharge_rate': 0.25})\n",
    "td_20_TEST = env_20.reset(batch_size=[100]).to(device)\n",
    "\n",
    "\n",
    "enforce_reproducibility(0)\n",
    "env_50 = EVRPTWEnv(generator_params={'num_loc': 50, \n",
    "                                     'num_station': 6,\n",
    "                                     'vehicle_limit': 6,\n",
    "                                     'vehicle_speed': 5,\n",
    "                                     'vehicle_capacity': vehicle_capacity,\n",
    "                                     'max_time': 1,\n",
    "                                     'horizon': 1,\n",
    "                                     'fuel_consumption_rate': 0.25,\n",
    "                                     'inverse_recharge_rate': 0.25})\n",
    "td_50_TEST = env_50.reset(batch_size=[100]).to(device)\n",
    "\n",
    "\n",
    "enforce_reproducibility(0)\n",
    "env_100 = EVRPTWEnv(generator_params={'num_loc': 100,\n",
    "                                    'num_station': 12,\n",
    "                                    'vehicle_limit': 12,\n",
    "                                    'vehicle_speed': 5,\n",
    "                                    'vehicle_capacity': vehicle_capacity,\n",
    "                                    'max_time': 1,\n",
    "                                    'horizon': 1,\n",
    "                                    'fuel_consumption_rate': 0.25,\n",
    "                                    'inverse_recharge_rate': 0.25})\n",
    "td_100_TEST = env_100.reset(batch_size=[100]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2b92290e4554f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T22:15:25.146517Z",
     "start_time": "2024-11-17T22:15:24.187177Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n",
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCH = 50\n",
    "BATCH_SIZE = 512\n",
    "TRAIN_DATA_SIZE = BATCH_SIZE * 200\n",
    "VAL_DATA_SIZE = BATCH_SIZE * 50\n",
    "# MAX_EPOCH = 2\n",
    "# BATCH_SIZE = 512\n",
    "# TRAIN_DATA_SIZE = BATCH_SIZE * 50\n",
    "# VAL_DATA_SIZE = BATCH_SIZE * 10\n",
    "\n",
    "# POMO\n",
    "policy1 = AttentionModelPolicy(env_name=env_10.name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "policy2 = AttentionModelPolicy(env_name=env_20.name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "policy5 = AttentionModelPolicy(env_name=env_50.name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "model_10 = POMO(env_10,\n",
    "                policy1,\n",
    "                 # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                  \"weight_decay\": 1e-6})\n",
    "\n",
    "model_20 = POMO(env_20,\n",
    "                policy2,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "model_50 = POMO(env_50,\n",
    "                policy5,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "\n",
    "# RCPOMO\n",
    "policy_c1 = AttentionModelPolicy(env_name=env_10.name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "policy_c2 = AttentionModelPolicy(env_name=env_20.name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "policy_c5 = AttentionModelPolicy(env_name=env_50.name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "model_c10 = RewardConstrainedPOMO(env_10,\n",
    "                policy_c1,\n",
    "                 # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                  \"weight_decay\": 1e-6})\n",
    "\n",
    "model_c20 = RewardConstrainedPOMO(env_20,\n",
    "                policy_c2,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "model_c50 = RewardConstrainedPOMO(env_50,\n",
    "                policy_c5,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "# RINFORCE\n",
    "policy_r1 = AttentionModelPolicy(env_name=env_10.name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "policy_r2 = AttentionModelPolicy(env_name=env_20.name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "policy_r5 = AttentionModelPolicy(env_name=env_50.name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "model_r10 = REINFORCE(env_10,\n",
    "                policy_r1,\n",
    "                 baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                  \"weight_decay\": 1e-6})\n",
    "\n",
    "model_r20 = REINFORCE(env_20,\n",
    "                policy_r2,\n",
    "                baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "model_r50 = REINFORCE(env_50,\n",
    "                policy_r5,\n",
    "                baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_data_size=TRAIN_DATA_SIZE,\n",
    "                val_data_size=VAL_DATA_SIZE,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83febd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [00:19<00:00, 10.07it/s, v_num=513, train/reward=-10.3, train/loss=0.866, val/reward=-4.99]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [00:19<00:00, 10.00it/s, v_num=513, train/reward=-10.3, train/loss=0.866, val/reward=-4.99]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [00:19<00:00, 10.26it/s, v_num=514, train/reward=-10.8, train/loss=0.0159, val/reward=-4.88]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [00:19<00:00, 10.19it/s, v_num=514, train/reward=-10.8, train/loss=0.0159, val/reward=-4.88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | WarmupBaseline       | 3.6 M  | train\n",
      "----------------------------------------------------------\n",
      "7.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.1 M     Total params\n",
      "28.482    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "124       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [00:35<00:00,  5.59it/s, v_num=515, train/reward=-4.14, train/loss=0.0199, val/reward=-4.14]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [00:50<00:00,  4.00it/s, v_num=515, train/reward=-4.14, train/loss=0.0199, val/reward=-4.14]\n"
     ]
    }
   ],
   "source": [
    "td_scale_test = [td_10_TEST, td_20_TEST, td_50_TEST, td_100_TEST]\n",
    "env_scale = [env_10, env_20, env_50, env_100]\n",
    "scale = [10, 20, 50, 100]\n",
    "\n",
    "# POMO\n",
    "trainer_STEP1 = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy1.to(device),\n",
    "            test_data=td_scale_test,\n",
    "            env_scale=env_scale,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"HARD_POMO_C10\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer_STEP1.fit(model_10)\n",
    "\n",
    "# RCPOMO\n",
    "trainer_C_STEP1 = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c1.to(device),\n",
    "            test_data=td_scale_test,\n",
    "            env_scale=env_scale,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"HARD_RCPOMO_C10\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer_C_STEP1.fit(model_c10)\n",
    "\n",
    "# REINFORCE\n",
    "trainer_R_STEP1 = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_r1.to(device),\n",
    "            test_data=td_scale_test,\n",
    "            env_scale=env_scale,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"HARD_REINFORCE_C10\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer_R_STEP1.fit(model_r10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d474c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POMO: Trained with Environment of C=10, S=3, EV=3\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 4.856230\n",
      "Scale: 20 | FeasibleCounts: 97 | Mean Trained Test Cost: 8.015352\n",
      "Scale: 50 | FeasibleCounts: 95 | Mean Trained Test Cost: 16.581690\n",
      "Scale: 100 | FeasibleCounts: 99 | Mean Trained Test Cost: 31.866270\n",
      "\n",
      "RCPOMO: Trained with Environment of C=10, S=3, EV=3\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 4.668255\n",
      "Scale: 20 | FeasibleCounts: 98 | Mean Trained Test Cost: 7.802932\n",
      "Scale: 50 | FeasibleCounts: 93 | Mean Trained Test Cost: 16.237083\n",
      "Scale: 100 | FeasibleCounts: 100 | Mean Trained Test Cost: 30.556526\n",
      "\n",
      "REINFORCE: Trained with Environment of C=10, S=3, EV=3\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 4.255553\n",
      "Scale: 20 | FeasibleCounts: 90 | Mean Trained Test Cost: 6.736508\n",
      "Scale: 50 | FeasibleCounts: 79 | Mean Trained Test Cost: 13.739327\n",
      "Scale: 100 | FeasibleCounts: 100 | Mean Trained Test Cost: 24.965090\n"
     ]
    }
   ],
   "source": [
    "policy1 = policy1.to(device)\n",
    "rewards_trained, num_valid = get_reward_and_check(policy1, td_scale_test, env_scale)\n",
    "# print(rewards_trained)\n",
    "print(\"POMO: Trained with Environment of C=10, S=3, EV=3\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_valid[i]} | Mean Trained Test Cost: {-rewards_trained[i].mean():3f}\")\n",
    "\n",
    "policy_c1 = policy_c1.to(device)\n",
    "rewards_c_trained, num_c_valid = get_reward_and_check(policy_c1, td_scale_test, env_scale)\n",
    "print(\"\\nRCPOMO: Trained with Environment of C=10, S=3, EV=3\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_c_valid[i]} | Mean Trained Test Cost: {-rewards_c_trained[i].mean():3f}\")\n",
    "    \n",
    "policy_r1 = policy_r1.to(device)\n",
    "rewards_r_trained, num_r_valid = get_reward_and_check(policy_r1, td_scale_test, env_scale)\n",
    "print(\"\\nREINFORCE: Trained with Environment of C=10, S=3, EV=3\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_r_valid[i]} | Mean Trained Test Cost: {-rewards_r_trained[i].mean():3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98a7e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer_STEP1, trainer_C_STEP1, trainer_R_STEP1\n",
    "del rewards_trained, rewards_c_trained, rewards_r_trained, num_valid, num_c_valid, num_r_valid\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a224fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [00:35<00:00,  5.71it/s, v_num=516, train/reward=-39.6, train/loss=-20.9, val/reward=-8.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [00:35<00:00,  5.69it/s, v_num=516, train/reward=-39.6, train/loss=-20.9, val/reward=-8.20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [01:33<00:00,  2.13it/s, v_num=517, train/reward=-51.2, train/loss=-22.1, val/reward=-8.50] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [01:34<00:00,  2.13it/s, v_num=517, train/reward=-51.2, train/loss=-22.1, val/reward=-8.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | WarmupBaseline       | 3.6 M  | train\n",
      "----------------------------------------------------------\n",
      "7.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.1 M     Total params\n",
      "28.482    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "124       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [01:10<00:00,  2.84it/s, v_num=518, train/reward=-12.2, train/loss=16.00, val/reward=-8.18]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [01:26<00:00,  2.32it/s, v_num=518, train/reward=-12.2, train/loss=16.00, val/reward=-8.18]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# POMO\n",
    "trainer_STEP2 = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy2.to(device),\n",
    "            test_data=td_scale_test,\n",
    "            env_scale=env_scale,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"HARD_POMO_C20\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer_STEP2.fit(model_20)\n",
    "\n",
    "# RCPOMO\n",
    "trainer_C_STEP2 = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c2.to(device),\n",
    "            test_data=td_scale_test,\n",
    "            env_scale=env_scale,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"HARD_RCPOMO_C20\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer_C_STEP2.fit(model_c20)\n",
    "\n",
    "# REINFORCE\n",
    "trainer_R_STEP2 = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_r2.to(device),\n",
    "            test_data=td_scale_test,\n",
    "            env_scale=env_scale,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"HARD_REINFORCE_C20\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer_R_STEP2.fit(model_r20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f5dd6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POMO: Trained with Environment of C=20, S=3, EV=3\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 5.261318\n",
      "Scale: 20 | FeasibleCounts: 100 | Mean Trained Test Cost: 8.114063\n",
      "Scale: 50 | FeasibleCounts: 99 | Mean Trained Test Cost: 16.827742\n",
      "Scale: 100 | FeasibleCounts: 99 | Mean Trained Test Cost: 33.060474\n",
      "\n",
      "RCPOMO: Trained with Environment of C=20, S=3, EV=3\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 5.336650\n",
      "Scale: 20 | FeasibleCounts: 99 | Mean Trained Test Cost: 8.125189\n",
      "Scale: 50 | FeasibleCounts: 100 | Mean Trained Test Cost: 16.944166\n",
      "Scale: 100 | FeasibleCounts: 100 | Mean Trained Test Cost: 33.477730\n",
      "\n",
      "REINFORCE: Trained with Environment of C=10, S=3, EV=3\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 5.012807\n",
      "Scale: 20 | FeasibleCounts: 99 | Mean Trained Test Cost: 8.085536\n",
      "Scale: 50 | FeasibleCounts: 100 | Mean Trained Test Cost: 16.917492\n",
      "Scale: 100 | FeasibleCounts: 100 | Mean Trained Test Cost: 31.081825\n"
     ]
    }
   ],
   "source": [
    "policy2 = policy2.to(device)\n",
    "rewards_trained, num_valid = get_reward_and_check(policy2, td_scale_test, env_scale)\n",
    "# print(rewards_trained)\n",
    "print(\"POMO: Trained with Environment of C=20, S=3, EV=3\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_valid[i]} | Mean Trained Test Cost: {-rewards_trained[i].mean():3f}\")\n",
    "\n",
    "policy_c2 = policy_c2.to(device)\n",
    "rewards_c_trained, num_c_valid = get_reward_and_check(policy_c2, td_scale_test, env_scale)\n",
    "print(\"\\nRCPOMO: Trained with Environment of C=20, S=3, EV=3\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_c_valid[i]} | Mean Trained Test Cost: {-rewards_c_trained[i].mean():3f}\")\n",
    "    \n",
    "policy_r2 = policy_r2.to(device)\n",
    "rewards_r_trained, num_r_valid = get_reward_and_check(policy_r2, td_scale_test, env_scale)\n",
    "print(\"\\nREINFORCE: Trained with Environment of C=10, S=3, EV=3\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_r_valid[i]} | Mean Trained Test Cost: {-rewards_r_trained[i].mean():3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b939bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer_STEP2, trainer_C_STEP2, trainer_R_STEP2\n",
    "del rewards_trained, rewards_c_trained, rewards_r_trained, num_valid, num_c_valid, num_r_valid\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8d3043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s, v_num=519, train/reward=-20.8, train/loss=-6.02, val/reward=-15.4] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [01:31<00:00,  2.18it/s, v_num=519, train/reward=-20.8, train/loss=-6.02, val/reward=-15.4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [01:33<00:00,  2.14it/s, v_num=520, train/reward=-22.8, train/loss=-6.94, val/reward=-15.3] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [01:33<00:00,  2.14it/s, v_num=520, train/reward=-22.8, train/loss=-6.94, val/reward=-15.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | WarmupBaseline       | 3.6 M  | train\n",
      "----------------------------------------------------------\n",
      "7.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.1 M     Total params\n",
      "28.482    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "124       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [00:55<00:00,  3.59it/s, v_num=521, train/reward=-21.3, train/loss=-145., val/reward=-17.0]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [01:04<00:00,  3.11it/s, v_num=521, train/reward=-21.3, train/loss=-145., val/reward=-17.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# POMO\n",
    "trainer_STEP5 = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy5.to(device),\n",
    "            test_data=td_scale_test,\n",
    "            env_scale=env_scale,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"HARD_POMO_C50\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer_STEP5.fit(model_50)\n",
    "\n",
    "# RCPOMO\n",
    "trainer_C_STEP5 = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c5.to(device),\n",
    "            test_data=td_scale_test,\n",
    "            env_scale=env_scale,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"HARD_RCPOMO_C50\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer_C_STEP5.fit(model_c50)\n",
    "\n",
    "# REINFORCE\n",
    "trainer_R_STEP5 = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_r5.to(device),\n",
    "            test_data=td_scale_test,\n",
    "            env_scale=env_scale,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"HARD_REINFORCE_C50\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer_R_STEP5.fit(model_r50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b9babf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POMO: Trained with Environment of C=50, S=6, EV=6\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 5.788095\n",
      "Scale: 20 | FeasibleCounts: 100 | Mean Trained Test Cost: 8.282652\n",
      "Scale: 50 | FeasibleCounts: 100 | Mean Trained Test Cost: 15.128916\n",
      "Scale: 100 | FeasibleCounts: 100 | Mean Trained Test Cost: 26.151745\n",
      "\n",
      "RCPOMO: Trained with Environment of C=50, S=6, EV=6\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 5.650996\n",
      "Scale: 20 | FeasibleCounts: 99 | Mean Trained Test Cost: 7.790233\n",
      "Scale: 50 | FeasibleCounts: 100 | Mean Trained Test Cost: 15.168052\n",
      "Scale: 100 | FeasibleCounts: 100 | Mean Trained Test Cost: 26.645964\n",
      "\n",
      "REINFORCE: Trained with Environment of C=50, S=6, EV=6\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 5.091000\n",
      "Scale: 20 | FeasibleCounts: 97 | Mean Trained Test Cost: 8.217063\n",
      "Scale: 50 | FeasibleCounts: 100 | Mean Trained Test Cost: 16.950056\n",
      "Scale: 100 | FeasibleCounts: 100 | Mean Trained Test Cost: 30.425636\n"
     ]
    }
   ],
   "source": [
    "policy5 = policy5.to(device)\n",
    "rewards_trained, num_valid = get_reward_and_check(policy5, td_scale_test, env_scale)\n",
    "# print(rewards_trained)\n",
    "print(\"POMO: Trained with Environment of C=50, S=6, EV=6\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_valid[i]} | Mean Trained Test Cost: {-rewards_trained[i].mean():3f}\")\n",
    "\n",
    "policy_c5 = policy_c5.to(device)\n",
    "rewards_c_trained, num_c_valid = get_reward_and_check(policy_c5, td_scale_test, env_scale)\n",
    "print(\"\\nRCPOMO: Trained with Environment of C=50, S=6, EV=6\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_c_valid[i]} | Mean Trained Test Cost: {-rewards_c_trained[i].mean():3f}\")\n",
    "    \n",
    "policy_r5 = policy_r5.to(device)\n",
    "rewards_r_trained, num_r_valid = get_reward_and_check(policy_r5, td_scale_test, env_scale)\n",
    "print(\"\\nREINFORCE: Trained with Environment of C=50, S=6, EV=6\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_r_valid[i]} | Mean Trained Test Cost: {-rewards_r_trained[i].mean():3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7275cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_scale_test = [td_10_TEST, td_20_TEST, td_50_TEST, td_100_TEST]\n",
    "env_scale = [env_10, env_20, env_50, env_100]\n",
    "scale = [10, 20, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "812b110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCH = 50\n",
    "BATCH_SIZE_100 = 128\n",
    "TRAIN_DATA_SIZE_100 = BATCH_SIZE_100 * 200\n",
    "VAL_DATA_SIZE_100 = BATCH_SIZE_100 * 50\n",
    "\n",
    "# POMO\n",
    "policy100 = AttentionModelPolicy(env_name=env_100.name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "model_100 = POMO(env_100,\n",
    "                policy100,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE_100,\n",
    "                train_data_size=TRAIN_DATA_SIZE_100,\n",
    "                val_data_size=VAL_DATA_SIZE_100,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "\n",
    "# RCPOMO\n",
    "policy_c100 = AttentionModelPolicy(env_name=env_100.name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "model_c100 = RewardConstrainedPOMO(env_100,\n",
    "                policy_c100,\n",
    "                # baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE_100,\n",
    "                train_data_size=TRAIN_DATA_SIZE_100,\n",
    "                val_data_size=VAL_DATA_SIZE_100,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})\n",
    "\n",
    "\n",
    "# REINFORCE\n",
    "policy_r100 = AttentionModelPolicy(env_name=env_100.name,\n",
    "                              embed_dim=256,\n",
    "                              num_encoder_layers=6,\n",
    "                              num_heads=8,)\n",
    "\n",
    "model_r100 = REINFORCE(env_100,\n",
    "                policy_r100,\n",
    "                baseline=\"rollout\",\n",
    "                batch_size=BATCH_SIZE_100,\n",
    "                train_data_size=TRAIN_DATA_SIZE_100,\n",
    "                val_data_size=VAL_DATA_SIZE_100,\n",
    "                optimizer_kwargs={\"lr\": 1e-4, \n",
    "                                \"weight_decay\": 1e-6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b742846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyosi\\anaconda3\\envs\\rl4co\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [02:18<00:00,  1.44it/s, v_num=526, train/reward=-21.2, train/loss=-1.02, val/reward=-20.8]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [02:18<00:00,  1.44it/s, v_num=526, train/reward=-21.2, train/loss=-1.02, val/reward=-20.8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | SharedBaseline       | 0      | train\n",
      "----------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 M     Total params\n",
      "14.241    Total estimated model params size (MB)\n",
      "126       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [02:19<00:00,  1.44it/s, v_num=527, train/reward=-21.6, train/loss=-1.14, val/reward=-21.0]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [02:19<00:00,  1.44it/s, v_num=527, train/reward=-21.6, train/loss=-1.14, val/reward=-21.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                 | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | env      | EVRPTWEnv            | 0      | train\n",
      "1 | policy   | AttentionModelPolicy | 3.6 M  | train\n",
      "2 | baseline | WarmupBaseline       | 3.6 M  | train\n",
      "----------------------------------------------------------\n",
      "7.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.1 M     Total params\n",
      "28.482    Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "124       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [02:11<00:00,  1.52it/s, v_num=528, train/reward=-25.1, train/loss=-80.5, val/reward=-23.4]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 200/200 [02:26<00:00,  1.36it/s, v_num=528, train/reward=-25.1, train/loss=-80.5, val/reward=-23.4]\n"
     ]
    }
   ],
   "source": [
    "# POMO\n",
    "trainer_STEP100 = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy100.to(device),\n",
    "            test_data=td_scale_test,\n",
    "            env_scale=env_scale,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"HARD_POMO_C100\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer_STEP100.fit(model_100)\n",
    "\n",
    "# RCPOMO\n",
    "trainer_C_STEP100 = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_c100.to(device),\n",
    "            test_data=td_scale_test,\n",
    "            env_scale=env_scale,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"HARD_RCPOMO_C100\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer_C_STEP100.fit(model_c100)\n",
    "\n",
    "# REINFORCE\n",
    "trainer_R_STEP100 = RL4COTrainer(\n",
    "    max_epochs=MAX_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=None,\n",
    "    callbacks=[\n",
    "        RewardLoggingCallback(\n",
    "            policy=policy_r100.to(device),\n",
    "            test_data=td_scale_test,\n",
    "            env_scale=env_scale,\n",
    "            scale = scale,\n",
    "            log_dir=\"logs\",  # Need to set the logs folder or else\n",
    "            file_name=\"HARD_REINFORCE_C100\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "trainer_R_STEP100.fit(model_r100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5f0dff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POMO: Trained with Environment of C=100, S=12, EV=12\n",
      "Scale: 10 | FeasibleCounts: 100 | Mean Trained Test Cost: 4.868367\n",
      "Scale: 20 | FeasibleCounts: 70 | Mean Trained Test Cost: 6.940489\n",
      "Scale: 50 | FeasibleCounts: 94 | Mean Trained Test Cost: 12.453379\n",
      "Scale: 100 | FeasibleCounts: 100 | Mean Trained Test Cost: 20.401237\n",
      "\n",
      "RCPOMO: Trained with Environment of C=100, S=12, EV=12\n",
      "Scale: 10 | FeasibleCounts: 99 | Mean Trained Test Cost: 4.809740\n",
      "Scale: 20 | FeasibleCounts: 77 | Mean Trained Test Cost: 6.937341\n",
      "Scale: 50 | FeasibleCounts: 94 | Mean Trained Test Cost: 12.568416\n",
      "Scale: 100 | FeasibleCounts: 100 | Mean Trained Test Cost: 20.684378\n",
      "\n",
      "REINFORCE: Trained with Environment of C=100, S=12, EV=12\n",
      "Scale: 10 | FeasibleCounts: 95 | Mean Trained Test Cost: 4.912994\n",
      "Scale: 20 | FeasibleCounts: 50 | Mean Trained Test Cost: 7.151945\n",
      "Scale: 50 | FeasibleCounts: 66 | Mean Trained Test Cost: 13.672751\n",
      "Scale: 100 | FeasibleCounts: 99 | Mean Trained Test Cost: 23.472595\n"
     ]
    }
   ],
   "source": [
    "policy100 = policy100.to(device)\n",
    "rewards_trained, num_valid = get_reward_and_check(policy100, td_scale_test, env_scale)\n",
    "# print(rewards_trained)\n",
    "print(\"POMO: Trained with Environment of C=100, S=12, EV=12\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_valid[i]} | Mean Trained Test Cost: {-rewards_trained[i].mean():3f}\")\n",
    "\n",
    "policy_c100 = policy_c100.to(device)\n",
    "rewards_c_trained, num_c_valid = get_reward_and_check(policy_c100, td_scale_test, env_scale)\n",
    "print(\"\\nRCPOMO: Trained with Environment of C=100, S=12, EV=12\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_c_valid[i]} | Mean Trained Test Cost: {-rewards_c_trained[i].mean():3f}\")\n",
    "    \n",
    "policy_r100 = policy_r100.to(device)\n",
    "rewards_r_trained, num_r_valid = get_reward_and_check(policy_r100, td_scale_test, env_scale)\n",
    "print(\"\\nREINFORCE: Trained with Environment of C=100, S=12, EV=12\")\n",
    "for i, s in enumerate(scale):\n",
    "    print(f\"Scale: {s} | FeasibleCounts: {num_r_valid[i]} | Mean Trained Test Cost: {-rewards_r_trained[i].mean():3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae3598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62355511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl4co",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
