from typing import Any, Union

import torch
import torch.nn as nn

from rl4co.data.transforms import StateAugmentation
from rl4co.envs.common.base import RL4COEnvBase
from rl4co.models.rl.reinforce.reinforce import REINFORCE
from rl4co.models.zoo.am import AttentionModelPolicy
from rl4co.utils.ops import gather_by_index, unbatchify
from rl4co.utils.pylogger import get_pylogger

log = get_pylogger(__name__)


class POMO(REINFORCE):
    """POMO Model for neural combinatorial optimization based on REINFORCE
    Based on Kwon et al. (2020) http://arxiv.org/abs/2010.16011.

    Note:
        If no policy kwargs is passed, we use the Attention Model policy with the following arguments:
        Differently to the base class:
        - `num_encoder_layers=6` (instead of 3)
        - `normalization="instance"` (instead of "batch")
        - `use_graph_context=False` (instead of True)
        The latter is due to the fact that the paper does not use the graph context in the policy, which seems to be
        helpful in overfitting to the training graph size.

    Args:
        env: TorchRL Environment
        policy: Policy to use for the algorithm
        policy_kwargs: Keyword arguments for policy
        baseline: Baseline to use for the algorithm. Note that POMO only supports shared baseline,
            so we will throw an error if anything else is passed.
        num_augment: Number of augmentations (used only for validation and test)
        augment_fn: Function to use for augmentation, defaulting to dihedral8
        first_aug_identity: Whether to include the identity augmentation in the first position
        feats: List of features to augment
        num_starts: Number of starts for multi-start. If None, use the number of available actions
        **kwargs: Keyword arguments passed to the superclass
    """

    def __init__(
        self,
        env: RL4COEnvBase,
        policy: nn.Module = None,
        policy_kwargs={},
        baseline: str = "shared",
        num_augment: int = 8,
        augment_fn: Union[str, callable] = "dihedral8",
        first_aug_identity: bool = True,
        feats: list = None,
        num_starts: int = None,
        **kwargs,
    ):
        self.save_hyperparameters(logger=False)

        if policy is None:
            policy_kwargs_with_defaults = {
                "num_encoder_layers": 6,
                "normalization": "instance",
                "use_graph_context": False,
            }
            policy_kwargs_with_defaults.update(policy_kwargs)
            policy = AttentionModelPolicy(
                env_name=env.name, **policy_kwargs_with_defaults
            )

        assert baseline == "shared", "POMO only supports shared baseline"

        # Initialize with the shared baseline
        super(POMO, self).__init__(env, policy, baseline, **kwargs)

        self.num_starts = num_starts
        self.num_augment = num_augment
        if self.num_augment > 1:
            self.augment = StateAugmentation(
                num_augment=self.num_augment,
                augment_fn=augment_fn,
                first_aug_identity=first_aug_identity,
                feats=feats,
            )
        else:
            self.augment = None

        # Add `_multistart` to decode type for train, val and test in policy
        for phase in ["train", "val", "test"]:
            self.set_decode_type_multistart(phase)

    def shared_step(
        self, batch: Any, batch_idx: int, phase: str, dataloader_idx: int = None
    ):
        td = self.env.reset(batch)
        n_aug, n_start = self.num_augment, self.num_starts
        n_start = self.env.get_num_starts(td) if n_start is None else n_start

        # During training, we do not augment the data
        if phase == "train":
            n_aug = 0
        elif n_aug > 1:
            td = self.augment(td)

        # Evaluate policy
        out = self.policy(td, self.env, phase=phase, num_starts=n_start)

        # Unbatchify reward to [batch_size, num_augment, num_starts].
        reward = unbatchify(out["reward"], (n_aug, n_start))

        # Training phase
        if phase == "train":
            assert n_start > 1, "num_starts must be > 1 during training"
            log_likelihood = unbatchify(out["log_likelihood"], (n_aug, n_start))
            self.calculate_loss(td, batch, out, reward, log_likelihood)
            max_reward, max_idxs = reward.max(dim=-1)
            out.update({"max_reward": max_reward})
        # Get multi-start (=POMO) rewards and best actions only during validation and test
        else:
            if n_start > 1:
                # max multi-start reward
                max_reward, max_idxs = reward.max(dim=-1)
                out.update({"max_reward": max_reward})

                if out.get("actions", None) is not None:
                    # Reshape batch to [batch_size, num_augment, num_starts, ...]
                    actions = unbatchify(out["actions"], (n_aug, n_start))
                    out.update(
                        {
                            "best_multistart_actions": gather_by_index(
                                actions, max_idxs, dim=max_idxs.dim()
                            )
                        }
                    )
                    out["actions"] = actions

            # Get augmentation score only during inference
            if n_aug > 1:
                # If multistart is enabled, we use the best multistart rewards
                reward_ = max_reward if n_start > 1 else reward
                max_aug_reward, max_idxs = reward_.max(dim=1)
                out.update({"max_aug_reward": max_aug_reward})

                if out.get("actions", None) is not None:
                    actions_ = (
                        out["best_multistart_actions"] if n_start > 1 else out["actions"]
                    )
                    out.update({"best_aug_actions": gather_by_index(actions_, max_idxs)})

        metrics = self.log_metrics(out, phase, dataloader_idx=dataloader_idx)
        return {"loss": out.get("loss", None), **metrics}


class RewardConstrainedPOMO(REINFORCE):
    """Reward-Constrained POMO Model with independent Lagrange multipliers for each constraint
    Based on Tessler et al. (2018) https://arxiv.org/abs/1805.11074."""

    def __init__(
        self,
        env: RL4COEnvBase,
        policy: nn.Module = None,
        policy_kwargs={},
        baseline: str = "shared",
        num_augment: int = 8,
        augment_fn: Union[str, callable] = "dihedral8",
        first_aug_identity: bool = True,
        feats: list = None,
        num_starts: int = None,
        constraint_thresholds: dict = {"time": 1.0, "battery": 1.0, "cargo": 1.0},
        lambda_lrs: dict = {"time": 5e-5, "battery": 5e-5, "cargo": 5e-5},
        **kwargs,
    ):
        self.save_hyperparameters(logger=False)
        
        if policy is None:
            policy_kwargs_with_defaults = {
                "num_encoder_layers": 6,
                "normalization": "instance",
                "use_graph_context": False,
            }
            policy_kwargs_with_defaults.update(policy_kwargs)
            policy = AttentionModelPolicy(
                env_name=env.name, **policy_kwargs_with_defaults
            )
        
        assert baseline == "shared", "POMO only supports shared baseline"
        
        # Initialize with the shared baseline
        super(RewardConstrainedPOMO, self).__init__(env, policy, baseline, **kwargs)
        
        self.num_starts = num_starts
        self.num_augment = num_augment

        # Set constraint thresholds and learning rates for each lambda
        self.constraint_thresholds = constraint_thresholds
        self.lambda_lrs = lambda_lrs

        # Initialize dynamic Lagrange multipliers (lambdas) for each constraint type
        self.lambda1 = torch.tensor(1.0, requires_grad=False)  # Time window constraint
        self.lambda2 = torch.tensor(1.0, requires_grad=False)  # Battery constraint
        self.lambda3 = torch.tensor(1.0, requires_grad=False)  # Cargo capacity constraint
        
        if self.num_augment > 1:
            self.augment = StateAugmentation(
                num_augment=self.num_augment,
                augment_fn=augment_fn,
                first_aug_identity=first_aug_identity,
                feats=feats,
            )
        else:
            self.augment = None

        # Add `_multistart` to decode type for train, val and test in policy
        for phase in ["train", "val", "test"]:
            self.set_decode_type_multistart(phase)

    def shared_step(
        self, batch: Any, batch_idx: int, phase: str, dataloader_idx: int = None
    ):
        td = self.env.reset(batch)
        n_aug, n_start = self.num_augment, self.num_starts
        n_start = self.env.get_num_starts(td) if n_start is None else n_start

        # During training, we do not augment the data
        if phase == "train":
            n_aug = 0
        elif n_aug > 1:
            td = self.augment(td)

        # Evaluate policy
        out = self.policy(td, self.env, phase=phase, num_starts=n_start)

        # Unbatchify reward to [batch_size, num_augment, num_starts]

        # print('out["reward"]', out["reward"].shape)
        reward = unbatchify(out["reward"], (n_aug, n_start))
        # print('reward.shape', reward.shape)
        time_penalty = unbatchify(out["penalty_time"].squeeze(-1), (n_aug, n_start))
        battery_penalty = unbatchify(out["penalty_battery"].squeeze(-1), (n_aug, n_start))
        cargo_penalty = unbatchify(out["penalty_cargo"].squeeze(-1), (n_aug, n_start))
        total_penalty = (self.lambda1 * time_penalty +
                         self.lambda2 * battery_penalty +
                         self.lambda3 * cargo_penalty)
        
        penalized_reward = reward - total_penalty
        
        # Training phase
        if phase == "train":
            assert n_start > 1, "num_starts must be > 1 during training"
            log_likelihood = unbatchify(out["log_likelihood"], (n_aug, n_start))
            self.calculate_loss(td, batch, out, penalized_reward, log_likelihood)
            max_reward, max_idxs = penalized_reward.max(dim=-1)
            out.update({"max_reward": max_reward})

            # Update each lambda based on its specific penalty
            avg_penalty_time = time_penalty.mean().item()
            avg_penalty_battery = battery_penalty.mean().item()
            avg_penalty_cargo = cargo_penalty.mean().item()

            # Update lambda1 (time), lambda2 (battery), and lambda3 (cargo)
            self.lambda1 = max(0, self.lambda1 + self.lambda_lrs["time"] * (avg_penalty_time - self.constraint_thresholds["time"]))
            self.lambda2 = max(0, self.lambda2 + self.lambda_lrs["battery"] * (avg_penalty_battery - self.constraint_thresholds["battery"]))
            self.lambda3 = max(0, self.lambda3 + self.lambda_lrs["cargo"] * (avg_penalty_cargo - self.constraint_thresholds["cargo"]))
            
        # Get multi-start (=POMO) rewards and best actions only during validation and test
        else:
            if n_start > 1:
                # max multi-start reward
                max_reward, max_idxs = penalized_reward.max(dim=-1)
                out.update({"max_reward": max_reward})

                if out.get("actions", None) is not None:
                    # Reshape batch to [batch_size, num_augment, num_starts, ...]
                    actions = unbatchify(out["actions"], (n_aug, n_start))
                    out.update(
                        {
                            "best_multistart_actions": gather_by_index(
                                actions, max_idxs, dim=max_idxs.dim()
                            )
                        }
                    )
                    out["actions"] = actions

            # Get augmentation score only during inference
            if n_aug > 1:
                reward_ = max_reward if n_start > 1 else penalized_reward
                max_aug_reward, max_idxs = reward_.max(dim=1)
                out.update({"max_aug_reward": max_aug_reward})

                if out.get("actions", None) is not None:
                    actions_ = (
                        out["best_multistart_actions"] if n_start > 1 else out["actions"]
                    )
                    out.update({"best_aug_actions": gather_by_index(actions_, max_idxs)})

        metrics = self.log_metrics(out, phase, dataloader_idx=dataloader_idx)
        return {"loss": out.get("loss", None), **metrics}

    def compute_penalty(self, out):
        """Calculate penalties based on constraint violations."""
        # Retrieve the constraint violation information from the environment output
        penalty_time = out.get("penalty_time", torch.zeros_like(out["reward"]))
        penalty_battery = out.get("penalty_battery", torch.zeros_like(out["reward"]))
        penalty_cargo = out.get("penalty_cargo", torch.zeros_like(out["reward"]))

        # Store individual penalties for updating each lambda independently
        penalties = {
            "time": penalty_time,
            "battery": penalty_battery,
            "cargo": penalty_cargo,
        }

        # Calculate total penalty with current lambda parameters
        total_penalty = (
            self.lambda1 * penalty_time +
            self.lambda2 * penalty_battery +
            self.lambda3 * penalty_cargo
        )

        return total_penalty, penalties